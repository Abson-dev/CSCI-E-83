---
title: "Introduction to Linear Models"
author: "Steve Elston"
date: "02/28/2021"
output:
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("/usr/bin/python3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```

## today's agenda

- apply linear models to prediction problems
- interpret and evaluate linear models
- account for multi-colinearity in data

## Why linear models? 

Linearly models are widely used in statistics and machine learning   

- Understandable and interpretable    

- Generalize well, if properly fit     

- Highly scalable â€“ computationally efficient    

- Can approximate fairly complex functions    
   - A basis of understanding complex models     
   - Many non-linear models are at locally linear at convergence
   - We can learn a lot about the convergence of DL and RL models from linear approximations


## what is regression?

In statistics, **regression** refers to a family of model that attempt to predict the value of numeric random variable       

- Regression is a common form of a linear model        

- Linear regression is a building block of many statistical and ML methods:   
   - multivariate regression and principal component
   - Analysis of variance (ANOVA)
   - Polynomial regression
   - Logistic regression for binary classification
   - Neural networks (and deep learning)


## Terminology    

There are confusing diferennces between statistical and ML terminology      

| ML termenology | Statistical terminology          |
|--------------------------------|--------------------------------------|
| Regression vs classification   | Continuous numeric vs categorical response      |
| Learning algorithm or model    | Model                                |
| Features                         | Predictor or independent variables   |
| Target or label               | Response or dependent variables      |
| Training                       | Fitting                              |
| Trained model                  | Fitted model                         |
| Supervised learning            | Predictive modeling                |



## Formulating the Linear Model      

The general formulation of a linear model can be writen:     

$$\hat{y} = A \vec{b} + \vec{\epsilon}$$

- $\hat{y}$ is the dependent variable or label   
   - The value we are trying to predict   

- $A$ is the **model matrix**      
   - Defines the structure of the model    
   - Columns are values of the predictor variables or features 

- $\vec{b}$ is the vector model coefficients      
   - One coefficient for each predictor or feature  
   - Model is fit by finding an *optimal** value for each coefficient  

- $\epsilon$ is the **prediction error**   
   - The **residual** 
   - Is Normally distributed; $\epsilon \sim \mathcal{N}(0, \sigma^2)$


## Single Predictor Regression

Consider a simple case of regression with a single predictor   

- Only two coefficients defining a straight line            

$$
 \vec{b} = 
\begin{bmatrix}
\beta_0\\
\beta_1
\end{bmatrix}
$$

- $\beta_0 =$ the *intercept term**       
   - Intercept is value of $y$ at $x=0$       

- $\beta_1 =$ model *coefficient** for the predictor variable     
   - The **slope coefficient**    

- Given a variable predictor value $x_i$, the prediction, $\hat{y}_i$, is:

$$\hat{y}_i = \beta_0 + \beta_1 x_i$$



## The Model Matrix      

How do we create the model matrix   

- Start with a data table of $n$ samples         
   - First column is predictor variable    
   - Second column is the response variable   

$$
\begin{bmatrix}
x_1, y_1\\
x_2, y_2\\
x_3, y_3\\
\vdots, \vdots\\
x_n, y_n
\end{bmatrix}
$$

## The Model Matrix      

The **model matrix** for this case, including the intercept term is:   


$$
\begin{bmatrix}
1, x_1\\
1, x_2\\
1, x_3\\
\vdots, \vdots\\
1, x_n
\end{bmatrix}
$$

The column of 1's define the intercept term   

## Constructing the Model

For the $n$ data samples and the parameter vector $\vec{b}$ we can construct the entire model:

$$
\begin{bmatrix}
y_1\\
y_2\\
y_3\\
\vdots, \vdots\\
y_n
\end{bmatrix}
= 
\begin{bmatrix}
1, x_1\\
1, x_2\\
1, x_3\\
\vdots, \vdots\\
1, x_n
\end{bmatrix}
\begin{bmatrix}
\beta_0\\
\beta_1
\end{bmatrix}
+ 
\begin{bmatrix}
\epsilon_1\\
\epsilon_2\\
\epsilon_3\\
\vdots, \vdots\\
\epsilon_n
\end{bmatrix}
$$
Or

$$\hat{y} = A \vec{b} + \vec{\epsilon}$$


## Estiamting the Model Parmaters      

How do we find the best value for the coefficients      

- Need to minimize an **error metric**    

- Given training data, minimize the squared error between the prediction, $\hat{y}_i$, and the observed response variable or label, $y_i$:    

$$\min_{\vec{b}} \sum_i(y_i - A_{i,.} \hat{b})^2 = \min_{\vec{b}} \sum_i(y_i - \hat{y_i})^2 =  \min_{\vec{b}} \sum_i \epsilon_i$$

$A_{i,.}$ is the ith row of $A$

- Find $\vec{b}$ by minimizing the **sum of squared errors** is known as the **least squares** method  


## Estiamting the Model Parmaters      

We could try a naive solution:    

$$ \vec{b} = A^{-1}y$$

Where $A^{-1}$ is the **matrix inverse** of $A$     

- This *might work*, **BUT**     

- Even in the best case the direct algorithm has complexity $O(n^2)$, so inefficient  

## Estiamting the Model Parmaters   

We can use the **Normal equations**   

- Start with the problem     

$$\hat{y} = A \vec{b}$$

- Multiply by $A^T$     

$$A^TA \vec{b} = A^T \hat{y}$$

- Taking the inverse of $A^T A$ we arrive at the normal equations  

$$\vec{b} = (A^TA)^{-1}A^T \hat{y}$$

- $A^TA$ is the **covariance matrix** for the data set     
   - For our single predictor model this is just dimension 2x2
   - Much easier to take inverse   
   
   
## Example      

Let's start with a simulated data set with one predictor and one response variable 0

- The response variable, $y$ is linear in $x$ with additive random noise $\sim \mathcal{N}(0,1)$

- Intercept $= 0$ and slope $=1.0$

- The first 10 rows:

```{python, echo=FALSE}
import numpy as np
import numpy.random as nr
import pandas as pd
import statsmodels.formula.api as sm
from statsmodels.graphics.regressionplots import influence_plot, plot_regress_exog
import scipy.stats as ss
import matplotlib.pyplot as plt
import seaborn as sns

# Parameters of generated data
n_points = 50
x_start, x_end = 0, 10
y_start, y_end = 0, 10
y_sd = 1

# Generate data columns
nr.seed(5666)
x_data = np.linspace(x_start, x_end, n_points) # The x values
y_error = np.random.normal(loc=0, scale=y_sd, size=n_points) # The Normally distributed noise
y_data = np.linspace(y_start, y_end, n_points) + y_error + 1.0 # The y values including an intercept

# Put data in dataframe
sim_data = pd.DataFrame({'x':x_data, 'y':y_data})

sim_data.head(10)
```


## Example     

Plot these data

```{python, echo=FALSE}
# Matplotlib may give some font errors when loading for the first time, you can ignore 
plt.rc('font', size=12)
fig, ax = plt.subplots(figsize=(6, 5), ) 
ax.plot(sim_data['x'], sim_data['y'], 'ko')
plt.grid(True)
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('x vs y')
plt.ylim(0,11);
plt.show()
```

Notice the linear trend    


## Example

Fit the model using statsmodels.formula.api.ols to create a linear model object    

```{python}
## Define the regresson model and fit it to the data
ols_model = sm.ols(formula = 'y ~ x', data=sim_data).fit()

## Print the model coefficient
print('Intercept = %4.3f  Slope = %4.3f' % (ols_model._results.params[0], ols_model._results.params[1]))
```

Find the predicted values for each value of #x$:  

```{python}
# Add predicted to pandas dataframe
sim_data['predicted'] = ols_model.predict(sim_data.x)
# View head of data frame
sim_data.head(5)
```


## Example    

Plot the regression line against the original data    

```{python, echo=FALSE}
plt.rc('font', size=12)
fig, ax = plt.subplots(figsize=(6, 5), ) 
sns.lineplot(x='x', y='predicted', data=sim_data, color='red', ax=ax)
sns.scatterplot(x='x', y='y', data=sim_data, ax=ax)
ax.set_title('Scatter plot of original data with linear regression line')
_=ax.set_ylim(0,11)
plt.show()
```

This looks like a good fit, but how good is it really? 


## Evaluating Regression Models    

Evaluation of regression models focuses on the residuals or errors     

$$\vec{\epsilon} = \vec{y} - A\ \vec{b} = \vec{y} - \hat{y}$$

- Residuals should be Normally distributed with $0$ mean and constant variance    

$$\vec{\epsilon} \sim \mathcal{N}(0,\sigma)$$

- The residuals must be **homoskedastic** with respect to the fitted values    
    - Homoskedasitic residuals have constant variance with predicted values    
    
- Any trend or structure in the residuals indicates a poor model fit    
   - In these cases variance is not constant and we say these are **heteroskedasitc** residuals       
   - Heteroskedastic residuals indicate that model has not incorpoared all avaialble information   



## Evaluating Regression Models      

**Residual plots** is a key diagnostic for any regression model   

- Plot residual against the predicted values    

```{python, echo=FALSE}
# Add residuals to pandas dataframe
sim_data['resids'] = np.subtract(sim_data.predicted, sim_data.y)

def residual_plot(df):
    plt.rc('font', size=12)
    fig, ax = plt.subplots(figsize=(8, 3), ) 
    RMSE = np.std(df.resids)
    sns.scatterplot(x='predicted', y='resids', data=df, ax=ax)
    plt.axhline(0.0, color='red', linewidth=1.0)
    plt.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)
    plt.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0)
    plt.title('PLot of residuals vs. predicted')
    plt.xlabel('Predicted values')
    plt.ylabel('Residuals')
    plt.show()
    
residual_plot(sim_data)  
#sns.scatterplot(x='predicted', y='resids', data=sim_data)
#plt.show()
```

- These residuals look homoskedastic - we are happy! 


## Evaluating Regression Models   

Graphically test that the residuals are iid Normal   

```{python, echo=FALSE}
def plot_resid_dist(resids):
    plt.rc('font', size=12)
    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))
    ## Plot a histogram
    sns.histplot(x=resids, bins=20, kde=True, ax=ax[0])
    ax[0].set_title('Histogram of residuals')
    ax[0].set_xlabel('Residual values')
    ## Plot the Q-Q Normal plot
    ss.probplot(resids, plot = ax[1])
    ax[1].set_title('Q-Q Normal plot of residuals')
    plt.show()

plot_resid_dist(sim_data.resids)
```

- These plots look promising   


## Evaluating Regression Models  

We can quantitatively understand model performance by deffining these relationships     

- Start with the **sum of squares (total SST)** is the metric for the dependent variable     

- **Sum of squares residual(RSS)** is the fraction of SST not explained by the model   

- **Sum of squares explained (ESS)** is the faction of SST explained by the model
   
   
- Then, the following relationship must hold:   


$$SST = SSE + SSR$$   

Or, we can say that the sum of squares explained by the model is:   

$$SSE = SST - SSR$$

## Evaluating Regression Models    

Compute the sums of squares for the running example   

```{python}
y_bar = np.mean(sim_data.y)
SST = np.sum(np.square(np.subtract(sim_data.y, y_bar)))
SSR = np.sum(np.square(sim_data.resids))
SSE = SST - SSR
print('SST = {0:6.2f}'.format(TSS))
print('SSE = {0:6.2f}'.format(ESS))
print('SSR = {0:6.2f}'.format(RSS))
```

- The model has explained most of the TSS - 


## Evaluating Regression Models    



## linear regression: coefficients

Assume the response variable is $Y$ (numeric) and we have two predictor variables $X_1$ and $X_2$ (both numeric, we cover categorical variables later)

A **linear regression** model has the following general form

$$Y = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \text{error}$$

where $\beta_0$, $\beta_1$ and $\beta_2$ are the model's **parameters**, and we estimate them with $b_0$, $b_1$ and $b_2$, which are the **coefficients** in the below equation:

$$Y = b_0 + b_1 \cdot X_1 + b_2 \cdot X_2 + \text{error}$$

Note that by doing so, we add statistical uncertainty to the error term.
The data we collect is our **sample**, and once we **fit** a model on the data we get a **fitted model**, for example $b_0 = 2.91$, $b_1 = -1.02$ and $b_2 = 3.10$.



## linear regression: residuals

Once we have a fitted model, we can get predictions for any $X_1$ and $X_2$ values (not just the ones in the data) by plugging into the equation:

$$\hat Y = 2.91 - 1.02 \cdot X_1 + 3.10 \cdot X_2$$

Note how the error term disappeared, and we replace $Y$ with $\hat Y$, the **predicted value**.

The difference between the true value $Y$ and the predicted value $\hat Y$ is called the **residual** or **error**. We have one residual **per data point** (row) of labeled data.

We can also calculate the **sum of squared residuals** for the *whole* data using the following formula:

$$\text{SSE} = \sum_{i = 1}^n (Y_i - \hat Y_i)^2$$



## linear regression: visual intuition

- here we have only one predictor $X$ to keep the plot simple
- we **explain away** the variation in $Y$ using $X$, so $Y|X$ has less uncertainty than $Y$
- if we have a good model, residuals should look like "noise" (mean 0, constant variance)





## linear regression: matrix formulation

Let $\bold{X}$ be the **model matrix**, and $\bold{b}$ be the vector of coefficients, $\bold{Y}$ be the vector for the response variable, and $\bold{e}$ be the vector of residuals.

$$
\bold{X} = 
\begin{pmatrix}
1 & x_{11} & x_{12} \\
1 & x_{21} & x_{22} \\
\vdots & \vdots & \vdots \\
1 & x_{n1} & x_{n2}
\end{pmatrix},

\hspace{5pt} \bold{b} = (b_0, b_1, b_2),

\hspace{5pt} \bold{Y} = (y_1, y_2, \ldots, y_n)

$$

For linear regression, this is a $n \times (p+1)$ matrix where $n$ is the number of rows in the data, and $p$ the number of columns. $\bold{X}$ has an additional column of 1s to account for the intercept terms $b_0$. Then the equation for linear regression can be rewritten as

$$\bold{Y} = \bold{X}\bold{b} + \bold{e}$$


## linear regression: fitting

So how do we actually find $b_0$, $b_1$ and $b_2$?

we solve an **optimization problem**: we minimize $||\bold{Y} - \bold{\hat Y}||$ w.r.t the vector $\bold{b} = (b_0, b_1, b_2)$, which is why it is also called the **method of least squares**
- we can do this [analytically], solving the optimization to get a **closed-form solution**: 
  $$\bold{b} = (\bold{X'}\bold{X})^{-1}\bold{X'}\bold{Y}$$ 
- we can find a numerical solution to the optimization using **gradient descent**: start with a random choice for $\bold{b} = (b_0, b_1, b_2)$ and iteratively change the values in a way that reduces $||\bold{Y} - \bold{\hat Y}||$ until the results seem to converge, which happens when we reach the minimum; in practice, analytical solutions are hard to get, and even if they exist, the numerical solution is usually much more efficient

[analytically]: https://en.wikipedia.org/wiki/Least_squares



## linear regression: assumptions

- $Y$, $X_1$ and $X_2$ are numeric, but in lesson 10 we see how to also cover categorical variables and how to extent linear regression when $Y$ is categorical
- it is not necessary, but usually a good idea to **normalize** $X_1$ and $X_2$ before fitting
- the data points are **independent**, which means rows $i$ and $j$ of the data are independently, otherwise regression may not be appropriate; we use time series models when the data is **auto-correlated**
- the residuals are more or less normally distributed and with mean 0 and constant variance (a property also called **homoskedasticity**)
- the predictor variables are **not** strongly correlated among each other, if so we run into a problem called **multi-collinearity** which can cause instability



## linear regression: evaluation

Define the **explained sum of squares (ESS)**, **total sum of squares (TSS)**, and **residual sum of squares (RSS)** as

$$\text{ESS} = \sum \left( \hat Y_i - \bar Y_i \right)^2, \text{TSS} = \sum \left( Y_i - \bar Y \right)^2, \text{RSS} = \sum \left( Y_i - \hat Y \right)^2$$

with some basic algebra, it can be shown that

$$\text{TSS} = \text{ESS} + \text{RSS}$$

- $R^2 = \frac{\text{ESS}}{\text{TSS}}$ is called the **coefficient of determination** and represents the percentage of the variation in the $Y$ that is explain away by using $X$ to predict $Y$

- **RSS** divided by the number of data points gives **mean squared error (MSE)**, and the root of **MSE** is **RMSE**, which is like the average error of the model



## linear regression as building block

We can extend linear regression in a few different ways:

- add an **interaction term** between $X_1$ and $X_2$ will give us

  $$Y = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \beta_{12} \cdot X_1 \cdot X_2 + \text{error}$$

- add polynomial terms like $X_1^2$ and $X_2^2$ and and so on

  $$Y = \beta_0 + \beta_1 \cdot X_1 + \beta_2 \cdot X_2 + \beta_{11} \cdot X_1^2 + \beta{22} \cdot X_2^2 + \text{error}$$

we can combine both ideas to get **polynomial regression**, which allows us to capture non-linearity, but we should be conservative about adding such terms otherwise we can easily end up with a too complex model that is likely to **overfit**







## interpreting results

It is very important to know how to interpret the model's coefficients:

- the **intercept** is the average value of $Y$ when x is zero: this can have a good interpretation as long as $X = 0$ is corresponds to a value that makes sense (if $X$ is centered around mean prior to modeling, $X = 0$ correspond to being average)
- the **slope** is the change in $Y$ we should expect to see **on average**, if we increase $X$ by 1 unit: a negative slope corresponds to a decrease and a positive slope an increase in $Y$
- **p-values** are listed for each of the coefficients, to see which ones are significant, but if the data is large enough even small effects will have significant p-values



## best practices

- **Okam's razor**: If a simpler model can predict with similar accuracy as the more complex model, then we choose the simpler model. A model with too many parameters is said to be **over-parametrized**. In practice, we can apply this principle using **regularization** (more on this topic later).
- We must be careful with model evaluation: adding too many predictors can inflate the performance of a model. For example, **R-squared** will almost always increase with additional predictors, so instead we use performance measures such as the **adjusted R-squared**, **AIC** or **BIC** that penalize models with too many terms unless they really help to improve performance.
- If we expect a lot of non-linearity we can try more sophisticated models such as **support vector regression** or **neural networks** that are better alternatives to polynomial regression, but ease of interpretability is not their strength.

----------------------------------------------------------------

## the end
