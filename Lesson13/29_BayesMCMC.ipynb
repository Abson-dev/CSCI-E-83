{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Modeling and Markov Chain Monte Carlo\n",
    "\n",
    "### Data Science 520\n",
    "\n",
    "## Overview\n",
    "\n",
    "In a previous lesson we explored the basics of Bayesian parameter estimation. The methods we used are restricted to only simple models. This lesson introduces you to a general and flexible form of Bayesian modeling using the **Markov chain Monte Carlo (MCMC)** methods. MCMC methods can be extended to extremely complex models, including **Bayesian hierarchical models**.  \n",
    "\n",
    "![](../images/Flips.png)\n",
    "<center>Source explain xkcd</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software\n",
    "\n",
    "Most Bayesian software packages use efficient Markov chain Monte Carlo (MCMC) methods. The most widely used of these is [Stan](https://mc-stan.org/), named for mathematician Stanislaw Ulam. Stan also includes variational approximation methods. \n",
    "\n",
    "A powerful, and generally more user friendly, Python package is [PyMC3](https://docs.pymc.io/). We will use PyMC3 in this lesson. The documentation for PyMC3 includes an excellent [Getting Started Jupyter Notebook](https://docs.pymc.io/notebooks/getting_started.html) along with other tutorials. Additionally you can find a number of [example Jupyter notebooks](https://docs.pymc.io/nb_examples/index.html) for many application areas.  \n",
    "\n",
    "***\n",
    "**Note:** To run this notebook you must have installed [PyMC3](https://docs.pymc.io/notebooks/getting_started.html).\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Review of Bayes Theorem\n",
    "\n",
    "Recall Bayes theorem:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "Computing the normalization $P(B)$ is a bit of a mess. But fortunately, we don't always need the denominator. We can rewrite Bayes Theorem as:\n",
    "\n",
    "$$𝑃(𝐴│𝐵)=𝑘∙𝑃(𝐵|𝐴)𝑃(𝐴)$$\n",
    "\n",
    "Ignoring the normalizaton constant $k$, we get:\n",
    "\n",
    "$$𝑃(𝐴│𝐵) \\propto 𝑃(𝐵|𝐴)𝑃(𝐴)$$\n",
    "\n",
    "### Bayesian parameter estimation\n",
    "\n",
    "How to we interpret the relationships shown above? We do this as follows:\n",
    "\n",
    "$$Posterior\\ Distribution \\propto Likelihood \\bullet Prior\\ Distribution \\\\\n",
    "Or\\\\\n",
    "𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠│𝑑𝑎𝑡𝑎) \\propto 𝑃(𝑑𝑎𝑡𝑎|𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠) $$\n",
    "\n",
    "These relationships apply to the observed data distributions, or to parameters in a model (partial slopes, intercept, error distributions, lasso constant,…). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Sampling and Scalability\n",
    "\n",
    "Real-world Bayes models have large numbers of parameters, even into the millions. As a naive approach to Bayesian analysis would be to simply grid sample across the dimensions of the parameter space. However, grid sampling will not scale. To understand the scaling problem, do the following thought experiment, where each dimension is sampled 100 times:\n",
    "\n",
    "- For a 1-parameter model: $100$ samples.\n",
    "- For a 2-parameter model: $100^2 = 10000$ samples.\n",
    "- For a 3-parameter model: $100^3 = 10^5$ samples.\n",
    "- For a 100-parameter model: $100^{100} = 10^{102}$ samples. \n",
    "\n",
    "As you can see, the computational complexity of grid sampling has **exponential scaling** with dimensionality. Clearly, we need a better approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Markov Chain Monte Carlo\n",
    "\n",
    "Large-scale Bayesian models use a family of efficient sampling methods known as **Markov chain Monte Carlo sampling**. Rather that systematically sampling on a grid MCMC methods sample distributions randomly. While MCMC methods are computationally efficient, but requires effort to understand how it works and what to do when things go wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##!pip install pymc3\n",
    "import pymc3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What is a Markov process?\n",
    "\n",
    "As you might guess from the name, a MCMC sampling uses a chain of **Markov sampling processes**. A Markov process is a **stochastic process** that a makes transition from a current state, $x_t$, to some next state, $x_{t+1}$, with some probability $\\Pi$. A Markov process has **no dependency on past states**. We can summarize properties of a Markov process:   \n",
    "- The probability of transition from one state to another is parameterized by a matrix of probabilities, $\\Pi$, of dim N X N for N possible state transitions,  \n",
    "- $\\Pi$  only depends on the current state, $x_t$,     \n",
    "- The transition can be to current state.   \n",
    "\n",
    "Since a Markov transition process depends only on the current state and not the history, we say a Markov process is **memoryless**. We can express the sequence of a Markov transition processes as:\n",
    "\n",
    "$$P(X_{t + 1}| X_t = x_t, x_{t-1}, x_{t-2}, \\ldots, x_0) = p(X_{t + 1}| x_t)$$\n",
    "\n",
    "Notice that, since the Markov process is memoryless, the transition probability only depends on the current state, $x_t$. There is no dependency on any previous states, $\\{x_{t-1}, x_{t-2}, \\ldots, x_0 \\}$. \n",
    "\n",
    "For a system with $N$ possible states we can write the **transition probability matrix**, $\\Pi$, from one state to another as follows:\n",
    "\n",
    "$$\\Pi = \n",
    "\\begin{bmatrix}\n",
    "\\pi_{1,1} & \\pi_{1,2} & \\cdots & \\pi_{1, N}\\\\\n",
    "\\pi_{2,1} & \\pi_{2,2} & \\cdots & \\pi_{2,N}\\\\\n",
    "\\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "\\pi_{N,i} & \\pi_{N,2} & \\cdots & \\pi_{N,N}\n",
    "\\end{bmatrix}\\\\\n",
    "where\\\\\n",
    "\\pi_{i,j} = probability\\ of\\ transition\\ from\\ state\\ i\\ to\\ state\\ j\\\\\n",
    "and\\\\\n",
    "\\pi_{i,i} = probability\\ of\\ staying\\ in\\ state\\ i\\\\\n",
    "further\\\\\n",
    "\\pi_{i,j} \\ne \\pi_{j,i}\\ in\\ general\n",
    "$$\n",
    "\n",
    "Notice that the probability of transition does not depend on the previous state history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example of a Markov Process\n",
    "\n",
    "To make the foregoing more concrete let's construct a simple example. We will start with a system of 3 states, $\\{ x_1, x_2, x_3 \\}$. The transition matrix is:    \n",
    "\n",
    "$$\\Pi = \n",
    "\\begin{bmatrix}\n",
    "\\pi_{1,1} & \\pi_{1,2} & \\pi_{1,3}\\\\\n",
    "\\pi_{2,1} & \\pi_{2,2} & \\pi_{2,3}\\\\\n",
    "\\pi_{3,1} & \\pi_{3,2} & \\pi_{3,3}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.0 & 0.6\\\\\n",
    "0.2 & 0.3 & 0.4\\\\\n",
    "0.3 & 0.7 & 0.0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "There are some key points to notice in this transition probability matrix.   \n",
    "- The probabilities of transition from a state is given in each column. Necessarily, the probabilities in each column must add to 1.0.  \n",
    "- The probabilities of a transition to the same state are given along the diagonal of the matrix.   \n",
    "- Some transitions are not possible. These transitions have a probability of 0.0.    \n",
    "\n",
    "Let's apply this probability matrix to a set of three possible states. As an example, let the **state vector** represent being in the first state at time step $t$; $\\vec{x_t} = [1,0,0]$. After a state transition, we compute the probability of being in each of the three possible states at the next time step, $t+1$, as:  \n",
    "\n",
    "$$\\vec{x}_{t+1}  = \\Pi\\ \\vec{x}_t = \n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.0 & 0.6\\\\\n",
    "0.2 & 0.3 & 0.4\\\\\n",
    "0.3 & 0.7 & 0.0\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "1\\\\\n",
    "0\\\\\n",
    "0\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "0.5 \\\\\n",
    "0.2 \\\\\n",
    "0.3 \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "\n",
    "> **Exercise 29-1:**  Based on the transition probability matrix above, answer the following questions:   \n",
    "> 1. Which state cannot transition to the same state? \n",
    "> 2. What is the minimum number of state transitions required to transition from the second state (second column) to the first state (first column)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### From Markov process to Markov chain    \n",
    "\n",
    "So far, we have only discussed a single step Markov process. That is, the process for a single state transition. What happens when there is a series of transitions? A sequence of such transitions is known as a **Markov chain**. There are two major behaviors observed with Markov Chains:  \n",
    "1. **Episodic Markov chains** have a **terminal state**. The terminal state can only transition to itself. Once the system is in the terminal state, we say that the episode has ended. Episodic processes are not of direct interest here, and we will not pursue them further.   \n",
    "2. **Continuous Markov chains** have no terminal state and continue indefinitely, at least in principle. Continuous Markov chains sample probability distribution, and are ideal for estimating Bayesian posterior distributions.  \n",
    "\n",
    "As already indicated, a Markov chain comprises a number of state transitions, one after another. Consider a chain of $n$ state transitions, $\\{t_1, t_2, t_3, \\ldots, t_n \\}$. Each transition in this process has the probabilities given by the state transition matrix, $\\Pi$.            \n",
    "\n",
    "To estimate the probabilities of being in the states we use a special case known as a **stationary Markov chain**. We will not discuss the technical mathematical details here. Here we will just summarize the key result: Over a large number of time steps, the number of times the states are visited is proportional to the state probabilities. Starting with some initial state, $\\vec{x}_0$, we can write the relationship as a continuous Markov chain:   \n",
    "\n",
    "$$\\Pi\\ \\Pi\\ \\Pi\\ \\ldots \\Pi\\ \\vec{x}_t = \\Pi^n\\ \\vec{x}_t  \\xrightarrow[\\text{$n \\rightarrow \\infty$}]{} \\vec{p(x)}$$    \n",
    "\n",
    "Notice that in the above, we can find the probabilities of the states without ever actually knowing the values of the transition matrix, $\\Pi$. As long as we can repeatedly sample the stochastic Markov process, we can estimate the state probabilities. This is the key concept of Markov Chain Monte Carlo sampling. \n",
    "\n",
    "As we proceed with this lesson we will use stationary Markov chains to estimate posterior probabilities of Bayesian models. The method relies on the result that the state probabilities converge eventually. As a consequence, we can develop flexible methods for sampling posterior distributions using Markov chain Monte Carlo. In summary, the model is sampled using a Markov chain and the sampled distribution should converge to the posterior distribution.    \n",
    "\n",
    "**********************************\n",
    "> **Exercise 29-2:** Based on the foregoing, answer the following questions:     \n",
    "> 1. Explain how a Markov chain is constructed from a series of Markov transition processes.    \n",
    "> 2. After a large number of transitions, the number of times some state have been visited is shown below. Compute the probabilities of each of the states:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_visits = np.array([20000, 10000, 7000, 13000])\n",
    "\n",
    "## Include code to compute the probabilities below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC and the Metropolis-Hastings Algorithm\n",
    "\n",
    "Using the principle of Markov chains, a number of MCMC algorithms have been developed over time to sample posterior distributions of Bayesian models. The first MCMC sampling algorithm developed is the **Metropolis-Hastings (M-H) algorithm** (Metropolis et al. (1953), Hastings (1970)). This algorithm is often referred to as simply the Metropolis algorithm or the M-H algorithm. \n",
    "\n",
    "The M-H algorithm has the following steps to estimate the posterior density of the parameters:\n",
    "1. Pick a starting point in the parameter space. This choice is typically a bit arbitrary. \n",
    "2. Evaluate the posterior distribution according to the model. In other words, take a sample of the likelihood $P(data|parameters)$ and prior, $P(parameters)$, and then compute the posterior.\n",
    "3. Choose a nearby point in parameter space randomly and evaluate the posterior at this point. The new point in parameter space is selected randomly, using a sampling distribution. The Normal distribution is a common choice of sampling distributions. Other distribution have also been used. \n",
    "4. Use the following **decision rule to accept or reject** the new sample:\n",
    "  - If the likelihood, $p(data | parameters)$, of the new point is greater than your current point, accept new point and move there.\n",
    "  - If the likelihood of the new point is less than your current point, only accept with probability according to the ratio:  \n",
    "$$Acceptance\\ probability\\ = \\frac{p(data | new\\ parameters)}{p(data | previous\\ parameters)}$$.\n",
    "4. If the sample is accepted, compute the posterior density at the new sample point.\n",
    "5. Repeat steps 3, 4 and 5 many times.\n",
    "\n",
    "Eventually, this algorithms converges and the posterior distribution is estimated. The M-H random sampling algorithm is far more **sample efficient** than naive grid sampling. To build some intuition, consider that since the M-H algorithm probabilistically samples the parameter space we only need to visit a limited number of points, rather than sample an entire grid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have outlined the basic Metropolis-Hastings MCMC algorithm, let's examine some of its properties.\n",
    "- The M-H algorithm is **guaranteed to eventually converge** to the underlying distribution. But as a practical issue, convergence can be quite slow. The convergence can be too slow to be useful for complex problems with high-dimensional parameter spaces.   \n",
    "- If there is high **serial correlation** from one sample to the next in M-H chain converges slowly. In this case we say the Markov chain has low **sample efficiency**. \n",
    "- To ensure efficient convergence the algorithm must be ‘tuned’. The tuning involves finding a good dispersion parameter value for the state sampling distribution. This parameter determines the size of the jumps the algorithm makes in the parameter space. For example if we use Normal distribution we must pick the variance, $\\sigma^2$. If $\\sigma^2$ is too small, the chain will only search the space slowly, using small jumps. If $\\sigma^2$ is too big, there are large jumps which also slows convergence, since the sampling of high density regions will be less likely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-H algorithm example\n",
    "\n",
    "Let's make these concepts concrete, by trying a simple example. We will find a sample estimate of the probability density of a bivariate Normal distribution.     \n",
    "\n",
    "As a first step, lets plot a set of points with density determined by the bivariate Normal distribution. Execute the code below and examine the resulting plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_bi_variate(x, title='Draws from a bivariate Normal distribution'):\n",
    "    ## Plot bi-variable points\n",
    "    fig, ax = plt.subplots(figsize=(8,8)) \n",
    "    ax.scatter(x[:, 0], x[:, 1], alpha=.1)\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    _=ax.set_title(title)\n",
    "\n",
    "\n",
    "## Define the covariance and mean of the bivariate Normal. \n",
    "sigma = np.array([[1, .6], [.6, 1]])\n",
    "mu = np.array([.5, .5])\n",
    "## Sample 10,000 realizations from the bivariate Normal\n",
    "random_points = np.random.multivariate_normal(mean=mu, cov=sigma,  size=10000)\n",
    "\n",
    "## Plot the result\n",
    "plot_bi_variate(random_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This plot looks as expected. The density of the dots is proportional to the probability density. You can see the effect of the covariance structure in the elliptical shape of the cloud of points. \n",
    "\n",
    "As a next step, let's look at the density of the marginal distributions of the $X$ and $Y$ variables. The code in the cell below plots histogram and density plots of the marginals. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_marginals(x):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12,3)) \n",
    "    ax[0].hist(x[:, 0], density=True, bins=40)\n",
    "    ax[0].set_title('Marginal X distribution')\n",
    "    ax[0].set_xlabel('X')\n",
    "    ax[1].hist(x[:, 1], density=True, bins=40)\n",
    "    ax[1].set_title('Marginal Y distribution')\n",
    "    ax[1].set_xlabel('Y')  \n",
    "    \n",
    "plot_marginals(random_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Exercise 29-3:** Examine the plots above and answer the following questions:   \n",
    "> 1. Do these marginal distributions appear approximately Normal? \n",
    "> 2. Do these marginal distributions exhibit any noticeable skewness or heavy tails? Is this behavior to be expected? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to sample these data using the M-H MCMC algorithm. The code in the cell below performs the following operations:\n",
    "\n",
    "1. Compute the likelihood of the bi-variate Normal distribution. \n",
    "2. Initialize the chain.\n",
    "3. Initialize some performance statistics.\n",
    "4. Sample the likelihood of the data using the M-H algorithm.\n",
    "5. Plot the result.\n",
    "\n",
    "Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Calculate the likelihood of a vector `x` for a multivariate normal\n",
    "# distribution MVN(mu, sigma)\n",
    "def likelihood(x, mu, sigma):\n",
    "    return ss.multivariate_normal.pdf(x, mu, sigma)\n",
    "\n",
    "# Initialize the output array\n",
    "chain_length = 10000\n",
    "chain = np.zeros((chain_length,2))\n",
    "# where to start\n",
    "chain[0,:] = [4.0,-4.0]\n",
    "\n",
    "\n",
    "def M_H_sample(start, chain_length, x):\n",
    "    ## Evaluate the current position\n",
    "    current_likelihood = likelihood(start[0], mu, sigma)\n",
    "    # Keep track of how often we accept or reject a proposal\n",
    "    accept_count = 0\n",
    "    reject_count = 0\n",
    "\n",
    "    for i in range(chain_length-1): # chain length minus 1 because we already have a point (the starting point)\n",
    "        # Sample the direction of the move we'll propose\n",
    "        delta = nr.multivariate_normal([0, 0], np.diag([.1, .1]))\n",
    "        # Our new proposal point is our previous position plus the sampled move\n",
    "        proposed = chain[i,:] + delta\n",
    "        proposed_likelihood = likelihood(proposed, mu, sigma)\n",
    "        \n",
    "        ## Accept according to probability \n",
    "        ## Two cases, are taken care of by one if statement since the uniform\n",
    "        ## distribution is on the range [0-1], exceeding a random value is the \n",
    "        ## positive decision.  \n",
    "        if (nr.uniform() < (proposed_likelihood / current_likelihood)):\n",
    "            accept_count += 1\n",
    "            current_likelihood = proposed_likelihood\n",
    "            chain[i+1,:] = proposed\n",
    "        else:\n",
    "            chain[i+1,:] = chain[i,:]\n",
    "            reject_count += 1 \n",
    "    ## Return the result        \n",
    "    return accept_count, reject_count, chain            \n",
    "\n",
    "## Sample the data distribution\n",
    "accept_count, reject_count, chain = M_H_sample(chain, chain_length, x=[0,0])\n",
    "print(chain.shape)\n",
    "## Plot the result\n",
    "plot_bi_variate(chain, title='MCMC values for bivariate normal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the long 'tail' on the sampled distribution. This behavior arrises from the initial wandering of the Markov chain as it finds the high probability regions of the distribution. This period in which the Markov chain wanders is known as the **burn-in period**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Exercise 29-4:** You will now investigate the properties of the MCMC burn-in period. As a first start, create a scatter plot of the X, Y variables for the first 5% of the Markov chain.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Add your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine you plot and answer these questions. \n",
    "1. How would you describe the reason for the 'trail' of samples from the initial sample value?  \n",
    "2. What evidence do you see that the sampling is converging toward the actual distribution of these data values?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "> **Exercise 29-5:**  Next, you will plot the density of the marginal distribution of the MCMC samples beyond the 5% burn-in period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_marginals(chain[num_burnin:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your plots of the MCMC sample marginal distributions with the marginal distribution of the original data samples. Do the MCMC sample marginal distributions look reasonably similar to the marginal distributions of the original data samples?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "> **Exercise 29-6:** Next, you will compare the **Maximum a posteriori or MAP** point of the sampled marginal distributions to the original means for $X$ and $Y$. Using the [Numpy mean function](https://numpy.org/doc/stable/reference/generated/numpy.mean.html) compute an approximation of the MAP for the MCMC sampled distribution. Compute and print this mean for i. the full chain, ii. the burn-in period and iii. the rest of the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results of your MAP estimates to the original data with $X = 0.5$ and $Y = 0.5$ and answer these questions. \n",
    "1. Does the MAP estimate, excluding the burn-in period, appear to be a reasonable estimate for the original data sample? \n",
    "2. Why does excluding the burn-in period from the MAP estimate improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Convergence and sampling efficiency of MCMC\n",
    "\n",
    "Let's turn our attention to the convergence properties of the M-H MCMC sampler. While convergence of MCMC sampling to the underlying distribution generally occurs, it can be slow. Unfortunately, it is not unusual for convergence to be too slow to be of practical use. Further, in some pathological cases, convergence may not occur at all.  \n",
    "\n",
    "The **acceptance rate** and **rejection rate** are key convergence statistics for the M-H algorithm. A low acceptance rate and high rejection rate are signs of poor convergence. Likewise, too few rejections, indicate that the algorithm is not exploring the parameter space sufficiently. The trade-off between these statistics is controlled by the dispersion of the sampling distribution. This hyperparameter is generally determined by trial and error. Unfortunately, there are few useful rules of thumb one can use.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Acceptance rate = %.2f' % (accept_count / chain_length))\n",
    "print('Rejection rate = %.2f' % (reject_count / chain_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These statistics indicate good convergence with a reasonable rejection rate.\n",
    "\n",
    "Another way to evaluate the convergence of MCMC algorithms is to look at the **trace** of the samples. The trace is a plot of the sample value with sample number. The code in the cell below plots the trace for both the $x$ and $y$ samples, including the burn-in period. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_traces(x):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(12,6))\n",
    "    ax[0].plot(x[:, 0])\n",
    "    ax[0].set_title('X chain')\n",
    "    ax[0].set_ylabel('Value')\n",
    "    ax[1].plot(x[:, 1])\n",
    "    ax[1].set_title('Y chain')\n",
    "    ax[1].set_xlabel('Sample number')\n",
    "    ax[1].set_ylabel('Value')\n",
    "\n",
    "plot_traces(chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Examine these sample traces. The nature of these plots gives insight into the progression of the MCMC sampling. Notice that there is a significant excursion during the initial burn-in period. After the initial burn-in you can see that the sampling wanders around the mode of the distribution, as it should. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Exercise 29-7:** You will now look at a close-up view of the traces just after the burn-in period. Create the code in the cell below to make plots of samples 1000 to 2000. Execute your code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine your plots and answer the following questions:   \n",
    "> 1. Do the samples appear to be centered around the MAP in the densest part of the joint distribution, $P(X,Y)$? \n",
    "> 2. Do you consider the foregoing behavior ideal for the M-H algorithm, and why? \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's take a look at the autocorrelation of our MCMC samples. Execute the code in the cell below, which uses the [Pandas.plotting.autocorrelation_plot](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.plotting.autocorrelation_plot.html) function to display the autocorrelation of each chain.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "ax[0].set_title('Autocorrelation of X')\n",
    "pd.plotting.autocorrelation_plot(chain[1000:2000, 0], ax=ax[0])\n",
    "ax[0].set_title('Autocorrelation of Y')\n",
    "pd.plotting.autocorrelation_plot(chain[1000:2000, 1], ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the autocorrelation dies off fairly quickly with lag. We can relate sampling efficiency to the autocorrelation of the samples. Intuitively, uncorrelated samples provide maximum information on the distribution being sampled. But, if there is significant autocorrelation, the new information gathered per-sample will be reduced, perhaps greatly so.   \n",
    "\n",
    "We can compute an **effective sample size or ESS**. ESS is the ratio between the number of samples adjusted for the autocorrelation and the hypothetical number of uncorrelated samples. In other words, the ratio of actual vs. ideal sampling. For a sample of size $N$, and autocorrelation function at lag k$, $ACF(k)$,we compute the ESS as follows:\n",
    "\n",
    "$$ESS = \\frac{N}{1 + 2 \\sum_k ACF(k)}$$    \n",
    "\n",
    "If the autocorrelation is low, the number of effective samples is high. However, if there is significant autocorrelation the ESS will be significantly less than the actual number of samples.\n",
    "\n",
    "We will return to this concept with an example latter in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Other MCMC Sampling Algorithms\n",
    "\n",
    "Now that you have some experience with the Metropolis-Hastings MCMC algorithm, let's examine some other MCMC sampling methods. The Metropolis-Hastings algorithm is a useful tool. However, this algorithm can suffer from slow convergence for several reasons:\n",
    "\n",
    "- Samples from the M-H algorithm generally have a fairly high serial correlation, resulting in low ESS. \n",
    "- As already discussed, one must ‘tune’ the state selection probability distribution. For example, if we use a Normal sampling distribution we must pick $\\sigma$. If $\\sigma$ is too small, the chain will only search the space slowly, with small jumps. If $\\sigma$ is too big, the the jumps are too large, slowing convergence.\n",
    "\n",
    "As a result of these limimitations, quite a number of MCMC sampling methods have been proposed in a quest to improve sample efficiency. Here, we will only address a few widely used choices.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gibbs sampling\n",
    "\n",
    "The Gibbs sampler (Geman and Geman, 1984) is an improved MCMC sampler which speeds convergence. The Gibbs sampler is named for the 19th Century physicist Josiah Willard Gibbs and is inspired by statistical mechanics.   \n",
    "\n",
    "In contrast to the M-H algorithm, the Gibbs sampler samples each dimension of the parameter space sequentially in a round-robin manner. Whereas, the M-H algorithm attempts jumps across all dimensions of the parameter space. \n",
    "\n",
    "The basic Gibbs sampler algorithm has the following steps:\n",
    "\n",
    "1. For an N dimensional parameter space, $\\{ \\theta_1, \\theta_2, \\ldots, \\theta_N \\}$, find a random starting point. \n",
    "2. In order, $\\{1, 2, 3, \\ldots, N\\}$, assign the next dimension to sample, starting with dimension $1$.  \n",
    "3. Sample the marginal distribution of the parameter given the observations, $D$, and other parameter values: $p(\\theta_1|D, \\theta_2, \\theta_3, \\ldots, \\theta_N)$.\n",
    "3. Repeat steps 2 and 3 until convergence.    \n",
    "\n",
    "From this simplified description of the Gibbs sampling algorithm you can infer.\n",
    "\n",
    "- When compared to the Metropolis-Hastings algorithm, the Gibbs sampler reduces serial correlation through round-robin sampling. The update along each dimension is approximately orthogonal to the preceding sample dimensions.    \n",
    "- There are no tuning parameters since sampling is based on the marginals of the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No U-Turn Sampler   \n",
    "\n",
    "The PyMC3 package uses the No U-Turn Sampler (NUTS) MCMC algorithm. NUTS uses an alternative to proposing new samples with the Metropolis-Hastings acceptance criteria or exploring dimensions in a round-robin fashion as done for the Gibbs sampler. Instead, NUTS models the exploration as the movement of a particle through a field. In 2-dimensions, this field can be imagined to look like a hilly landscape. The high spots on the hills are the high density regions we want to sample the most. The field that guides the movement of the particle through the space is derived from the target probability distribution, such that the particle is drawn towards dense (high likelihood) regions of the space. This strategy directs the exploration of the space using the gradient, rather than using a random wandering behavior as we saw with the M-H MCMC algorithm.\n",
    "\n",
    "The NUTS sampler is based on an earlier idea, Hamiltonian Monte Carlo. Imagine a ball in a 2-dimensional hilly landscape. The ball follows the gradient, which might send it part of the way up one of the hills. These hills represent the highest density regions of a distribution which we wish to sample. The difficulty with this idea is that the ball will often simply roll back down the hill to a less interesting low-density region. The original HMC method requires 2 hyperparameters to control stopping criteria of the ball. Unfortunately, in practice, setting these hyperparameters proved tricky, which limited the usefulness of HMC in practice.  \n",
    "\n",
    "The NUTS sampler adds a simple heuristic to the HMC algorithm. Stop the ball from rolling, if it tries to turn around. In other words, the ball is prevented from making U-turns in the density space. The need for complex stopping criteria, and associated hyperparameters, is eliminated. As a result, NUTS is both a highly efficient sampler and easy to use. \n",
    "\n",
    "Why even discuss other samplers when we have the NUTS algorithm? Unfortunately, while NUTS works well in many common cases, it is not guaranteed to converge. For density functions with highly complex behavior, other samplers are required. In these cases, the Gibbs sampler is typically used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical modeling\n",
    "\n",
    "So far, we have only worked with models having a simple flat parameter structure. To extend to more complex models and fully employ the power of Bayesian methods, we must use **hierarchical models**. These models allow us to compute posterior distributions, accounting for dependency in the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule of probability\n",
    "\n",
    "As the name implies, hierarchical Bayesian models use a hierarchy of conditional probability distributions. Creating the required hierarchy employs the **chain rule of probability** to **factor a joint distribution**. Recall a basic relationship for conditional probability for the point distribution of two random variables, $A$ and $B$:  \n",
    "\n",
    "$$P(A,B) = P(A|B)P(B)$$\n",
    "\n",
    "We can expand this relationship to a joint distribution of $n$ random variables, $A_1, A_2, A_3, A_4 \\ldots, A_n$:\n",
    "\n",
    "$$P(A_1, A_2, A_3, A_4 \\ldots, A_n) = P(A_1 | A_2, A_3, A_4, \\ldots, A_n)\\ P(A_2, A_3, A_4 \\ldots, A_n)$$\n",
    "\n",
    "In words, a joint distribution can be factored as a distribution of one of the variable, conditioned on the other variables, multiplied by the joint distribution of the other variables. \n",
    "\n",
    "We can continue this factorization until we reach an end point:\n",
    "\n",
    "$$P(A_1, A_2, A_3, A_4 \\ldots, A_n) = P(A_1 | A_2, A_3, A_4, \\ldots, A_n)\\ P(A_2 | A_3, A_4 \\ldots, A_n)\\ P(A_3| A_4 \\ldots, A_n) \\ldots P(A_n)$$\n",
    "\n",
    "> **Note:** The factorization is not unique. We can factor the variables in any order. In fact, for a joint distribution with $n$ variables, there are $n!$ unique factorizations. For example, we can factorize the foregoing distribution as:\n",
    "\n",
    "$$P(A_1, A_2, A_3, A_4 \\ldots, A_n) = P(A_n | A_{n-1}, A_{n-2}, A_{n-3}, \\ldots, A_1)\\ P(A_{n-1}| A_{n-2}, A_{n-3}, \\ldots, A_1)\\ P(A_{n-2}| A_{n-3}, \\ldots, A_1) \\ldots p(A_1)$$     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization applied to hierarchical models   \n",
    "\n",
    "We can apply the chain rule of probability to Bayes theorem. This approach allows us to create hierarchical Bayesian models. The hierarchy is determined by the factorization of the probability distribution used.     \n",
    "\n",
    "To make the foregoing concrete, let's consider an example of a multi-parameter model; estimating both the mean, $\\mu$, and standard deviation, $\\sigma$, of a Normal distribution. For this two-parameter model, we can write Bayes theorem like this:\n",
    "\n",
    "$$p(\\mu, \\sigma | D) \\propto p(D| \\mu, \\sigma) p(\\mu, \\sigma)$$\n",
    "\n",
    "This form looks a bit daunting since the likelihood and the prior both involved two parameters. Fortunately, we can simplify this relationship by applying the chain rule of probability: \n",
    "\n",
    "\\begin{align}\n",
    "p(\\mu, \\sigma | D) &\\propto p(D | \\mu) p(\\mu | \\sigma) p(\\sigma)\\\\\n",
    "&\\propto\\ Likelihood\\ *\\ Prior\\ of\\ \\mu\\ given\\ \\sigma\\ *\\ Prior\\ of\\ \\sigma\n",
    "\\end{align}\n",
    "\n",
    "As you can see, a complex multi-parameter Bayesian model is transformed into a hierarchy. The factorization has two likelihood functions, each with only a single parameter; the mean and the scale or variance. There are now two priors as well. One prior is a conditional distribution.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gamma distribution\n",
    "\n",
    "For the prior of the standard deviation, we will be using the Gamma distribution. You may not be familiar with the [Gamma distribution](https://en.wikipedia.org/wiki/Gamma_distribution). We will give a brief introduction here.   \n",
    "\n",
    "There are several ways one can parameterize the Gamma distribution. One method is to specify a shape parameter, $a$ and a scale parameter, $b$. The special case of $a=1$ is the exponential distribution.  \n",
    "\n",
    "The Gamma distribution is defined on the range $0 > Gamma(a,b) > \\infty$. This range is a desirable property for a prior, since scale parameters must have values $>0$.   \n",
    "\n",
    "The code in the cell below displays the Gamma distribution for several values of the shape parameter, $a$, with fixed scale parameter, $b=2.0$. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "a_list=[1.0,2.0,3.0,4.0]\n",
    "x = np.linspace(ss.gamma.ppf(0.001, a_list[0]),\n",
    "                ss.gamma.ppf(0.999, a_list[0]), 1000)\n",
    "for a in a_list:\n",
    "    ax.plot(x, ss.gamma.pdf(x, a, scale=2.0),\n",
    "        lw=2, alpha=0.6, label='a = ' + str(a))\n",
    "ax.set_title('Density of Gamma distribution')    \n",
    "_=ax.legend()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a MCMC hierarchical Bayes Model\n",
    "\n",
    "Given the foregoing factorization of the posterior distribution, we will now define an **hierarchical Bayes model**. The model is considered hierarchical since the quantity we really want to know, the posterior distribution depends on the distribution of two model parameters. At first glance, it might seem some direct method of computing the posterior distribution can be easily found. But, this is not the case. Hence, we resort to finding an approximate solution using MCMC methods. \n",
    "\n",
    "The structure of the hierarchical model is shown in the figure below. The priors for the mean, $\\mu$, and standard deviation, $\\sigma$ are shown along with the Normal posterior.\n",
    "\n",
    "<img src=\"../images/HierarchicalModel.png\" alt=\"Drawing\" style=\"width:550px; height:300px\"/>\n",
    "<center> Hierarchical model for the posterior distribution </center>\n",
    "\n",
    "In mathematical terms we can define the hierarchical model as follows:\n",
    " \n",
    "1. The prior of the scale, $\\sigma$, is the Gamma distribution: $Gamma(2, 2)$. We choose this prior since it is fairly uninformative and we can imagine not having much prior information in this case.  \n",
    "2. The prior distributions of the mean, $\\mu$, is modeled as Normal distribution: $N(0, 3)$. The Normal is its own conjugate distribution. Further, this prior, with a large scale parameter, is fairly uninformative and will not restrict the posterior distribution values. \n",
    "3. The posterior distribution is defined using the priors for $\\mu$ and $\\sigma$ and the Normal likelihood.   \n",
    "  \n",
    "The code in the cell below creates a Normally distributed random sample. Execute the code and examine the histogram.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up the data set as a regression problem\n",
    "nr.seed(2233)\n",
    "N = 1000\n",
    "observations = nr.normal(loc=2.0, scale=2.0, size=N)\n",
    "\n",
    "plt.hist(observations, bins=40)\n",
    "_ = plt.title('Synthetic data for hierarchical Bayes problem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the structure of the model we will create the model in code using the PyMC3 package. The code in the cell below does so by the following steps:\n",
    "1. A [PyMC3 model object](https://docs.pymc.io/api/model.html) is created. This object serves as a container for the model and the MCMC samples. \n",
    "2. The prior distributions of the scale, $\\sigma$, and location, $\\mu$ are defined. \n",
    "3. A model for the posterior distribution is defined.  \n",
    "\n",
    "Execute the code to create the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(y):\n",
    "    ## define a PyMC3 model object\n",
    "    model = pymc3.Model()\n",
    "\n",
    "    with model:\n",
    "        # Define the prior distributions for the two model parameters, location, mu and scale\n",
    "        scale = pymc3.Gamma('scale', alpha=2,beta=2)\n",
    "        mu = pymc3.Normal('mu', mu=0, sigma=3.0)\n",
    "        \n",
    "        # The model for the posterior distribution uses the priors of the location and scale\n",
    "        # to model the observations.   \n",
    "        y_obs = pymc3.Normal('y_obs', mu=mu, sigma=scale, observed=y)\n",
    "    return model\n",
    "\n",
    "model = create_model(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model defined, you can now compute the MCMC samples. Execute the code in the cell below to compute the [samples](https://docs.pymc.io/api/inference.html#module-pymc3.sampling) with PyMC3.\n",
    "\n",
    "*******************\n",
    "> **Computational note:** This example problem is fairly simple and the MCMC chains converge quickly. As a result only 1,000 samples per chain are required. For more complex problems, 10,000 or more samples per chain may be required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will use 4 chains with 1000 samples each\n",
    "num_chains = 4\n",
    "Nsamples = 1000\n",
    "\n",
    "## Compute the samples using the default NUTS sampler.   \n",
    "with model:\n",
    "    samples = pymc3.sample(Nsamples, chains=num_chains)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the cell below plots the posterior density estimates and traces for the 4 MCMC chains for each of the model parameters. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plot the traces and the density estimates from the 4 MCMC chains\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    _ = pymc3.plots.traceplot(samples) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> **Exercise 29-8:** Examine these plots and answer the following questions:\n",
    "> 1. The trace plots on the right show the path of the 4 MCMC chains for the $a$ and $b$ model parameters. Do the traces of all 4 chains look similar in terms of convergence? \n",
    "> 2. The density for the $a$ and $b$ model parameters are shown on the left. Do these posterior distributions seem compatible with the known values of the distribution parameters used to generate the data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the solution  \n",
    "\n",
    "Now that the MCMC sampling of the model is complete, let's evaluate the results. PyMC3 provides quite a number of model evaluation statistics. These can be found in the [Stats section of the PyMC3 documentation](https://pymc3.readthedocs.io/en/latest/api/stats.html).   \n",
    "\n",
    "As a first step, execute the code in the cell below to display a summary of the samples of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymc3.summary(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary shows a lot of useful information:\n",
    "    \n",
    "1. The mean of the posterior distribution of each coefficient value. \n",
    "2. The standard deviation (`sd`) of the mean of the posterior distribution of the coefficient values. \n",
    "3. The upper and lower bonds of the 96% **Highest Probability Density (HPD)** interval estimate for the posterior distribution of the coefficients. HPD is an alternative term for the Highest Density Interval (HDI) or the credible interval.  \n",
    "4. The msce_mean is an estimate of the error arising from the MCMC sampling itself. The msce_sd is standard deviation of this mean. Ideally, these quantities should be small with respect to the mean value of the posterior distribution. The MCMC sampling should add only minimal noise to the posterior distribution.  \n",
    "5. Next come four metrics of **effective sample size (ESS)**. We have already discussed the concept of effective sample size. These metrics give indications of overall ESS alone with specific metrics for the bunk of the posterior distribution and its tails. These statistics are reported with respect to the total number of samples in all chains.    \n",
    "6. The **Gelman-Rudin statistic** (`Rhat`) (Gelman and Rubin, 1992) measures the ratio of the **variance shrinkage between chains** to the **variance shrinkage within chains**. The Gelman-Rudin statistic should converge to 1.0. That is, if all chains converge, the reduction in variance between chains and within the chains should be the same.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 29-9:** Given the statistics for the MCMC samples and the foregoing explanation answer the following questions:   \n",
    "> 1. Given the HPD and standard deviation how credible are the estimates of the means of the posterior distributions of the coefficients?   \n",
    "> 2. Given the ESS statistics how efficient does the MCMC samples appear to be?   \n",
    "> 3. Given the Gelman-Rudin statistic, does it appear that the Monte Carlo chains have converged?     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will examine the HDI (or HPD) of the parameter estimates for each of the Markov chains for each of the parameters. We will examine this in two ways. First, the evolution of the interval with sampling. Second, by aggregated over the sampling history.      \n",
    "\n",
    "The code in the ell below creates plots of the mean of the posterior distribution and the HDI for each parameter during the evolution of each Monte Carlo chain. Execute the code and examine the results.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace_variable(samples, varnames, quantiles=(0.025, 0.5, 0.975)):\n",
    "    for i, chain in enumerate(samples.chains):\n",
    "        for j, varname in enumerate(varnames):\n",
    "            plt.subplot(len(samples.chains), len(varnames), i * len(varnames) + j + 1)\n",
    "            values = pd.Series(samples.get_values(varname, chains=chain))\n",
    "            for q in quantiles:\n",
    "                plt.plot(\n",
    "                    values.expanding(min_periods=1).quantile(q), \n",
    "                    label='%.3f' % q)\n",
    "            plt.title('var=%s, chain=%d' % (varname, chain))\n",
    "            plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 12))\n",
    "trace_variable(samples, ['mu', 'scale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To gain another view of the posterior distribution and the HDI, execute the code in the cell below to display range plots for each Markov chain and for each variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_=pymc3.forestplot(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 29-10:** \n",
    "> Examine the results in the plots and answer the following questions:    \n",
    "> 1. Do the Monte Carlo chains appear to converge quickly?   \n",
    "> 2. Are the HDIs consistent across the Monte Carlo chains?   \n",
    "> 3. Are the known parameters used to generate the data within the HDI of the Monte Carlo chains?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we already discussed, the convergence of MCMC algorithms is slowed by **autocorrelation** between the samples. PyMC3 provides the `autocorrplot()` function for examining the autocorrelation in Markov chains. The function creates autocorrelation function plots for each parameter and chain combination. \n",
    "\n",
    "To examine the results for the Markov chains, execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pymc3.autocorrplot(samples, max_lag=25, figsize=(10, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these plots and notice the low autocorrelations. These are unusually low, since the problem was particularly simple and the parameters easy to estimate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lesson you have done the following:\n",
    "\n",
    "- Reviewed the basic properties of a Markov process and Markov chains.\n",
    "- Performed a simple Markov chain Monte Carlo using the Metropolis-Hastings algorithm.\n",
    "- Created and computed a hierarchical Bayes model using No U-Turn Sampling in the PyMC3 package.\n",
    "- Evaluated the convergence of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Copyright 2017, 2018, 2019, 2020, 2021 Stephen F Elston. All rights reserved.\n",
    "\n",
    "Rights to use and modify this material on an unlimited and perpetual basis are granted to the University of Washington for use only by the University of Washington.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
