{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 26 - Nonlinear Regression Models\n",
    "\n",
    "### Steve Elston\n",
    "\n",
    "Until now, we have only worked with Normally distributed **numeric response variables**. However, **nonlinear response variables** are extremely common. For example, **categorical responses** and responses of counts from **point processes**.     \n",
    "\n",
    "In this notebook you will be introduced to working with models with nonlinear response variable. Specifically, you will work with the **generalized linear model (GLM)**. The GLM transforms a nonlinear response to a linear, or Normally distributed response using a **link function**. Link functions exist for any response distribution which can be written in exponential form.        \n",
    "\n",
    "We solve the nonlinear problem by creating a solution to a linear model. The nonlinear response is then computed using the **inverse link function**.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data Set\n",
    "\n",
    "In this section we will prepare the data set used in this notebook. These data were compiled from credit data for German bank customers, and are from the [University of California Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data). The objective is to build a model to predict customer credit quality, [good,bad], using the variable provided. This is a notoriously difficult problem.    \n",
    "\n",
    "> **Exercise 26-1:** The data sample is based on the outcome of customers fulfilling their repayment obligations on loans. However, as is typically the case, we have no information on how perspective customers who were turned down for loads and therefore have no payment history. The outcomes for these people are **unobservable**. Write a few sentences discussing how any model fitted with these data could lead to biased credit decisions.    \n",
    "\n",
    "As a first step, execute the code in the cell below to import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as ss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import sklearn.metrics as sklm\n",
    "from patsy import dmatrices\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to display the dimensions of the data frame and display the data types of the columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data = pd.read_csv('../data/German_Credit_Clean.csv')\n",
    "print(credit_data.shape)\n",
    "credit_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of numeric and categorical predictor variables. The label is binary with values $[0,1]$, of type 'int64'. We will discuss the properties of this label further shortly.  \n",
    "\n",
    "One difficulty with this data set is that there is an **imbalance** between the negative cases and the positive cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(credit_data.loc[:,'bad_credit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of positive cases is only 30% of all cases. This imbalance will bias the training of any model toward the negative cases.\n",
    "\n",
    "Now, we will prepare training and test data sets with normalized numeric features (predictor variables). Execute this code to create Bernoulli sampled training and test data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['loan_duration_mo','loan_amount','payment_pcnt_income','age_yrs']\n",
    "credit_data.loc[:,num_cols] = normalize(credit_data.loc[:,num_cols])\n",
    "\n",
    "## Create a mask and use it to split the data into a train and test set   \n",
    "nr.seed(6665)\n",
    "mask = nr.choice(credit_data.index, size = 600, replace=False)\n",
    "credit_data_train = credit_data.iloc[mask,:]\n",
    "credit_data_test = credit_data.drop(mask, axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Until now, we have been working strictly with linear regression models. Linear regression models have a numeric label. Further, ordinary linear regression assumes the label values are Normally distributed. \n",
    "\n",
    "But, what if the label has another distribution, particularly if the label is categorical? Using a more general form for the label values leads us to the **generalized linear model**. Generalized linear model can use a number of distributions for the label .   \n",
    "\n",
    "Here, we will look at a widely used variation of the generalized linear model using a **Binomial distribution**. This method is commonly known as **logistic regression**.\n",
    "\n",
    "Logistic regression is widely used as a classification model. Logistic regression is a linear model, with a binary response or label values, `{False, True}` or `{0, 1}`.  specifically, the response is computed as a log likelihood, leading to a Binomial distribution of the label values. \n",
    "\n",
    "The response of the linear model is transformed to the log likelihood using a sigmoidal function, also know as the **logistic function** or **logit**:\n",
    "\n",
    "$$f(x) = \\frac{1}{1 + e^{-\\kappa(x - x_0)}} \\\\\n",
    "\\kappa = steepness$$\n",
    "\n",
    "Execute the code in the cell below to compute and plot an example of the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the logistic transformation function (f(x) above)\n",
    "x_seq = np.linspace(-7, 7, 100)\n",
    "\n",
    "def log_fun(x, center=0, scale=1):\n",
    "    e = np.exp(scale*(center-x))\n",
    "    log_out = 1./(1. + e)\n",
    "    return log_out\n",
    "\n",
    "log_fun_vectorized = np.vectorize(log_fun)\n",
    "\n",
    "log_y = log_fun_vectorized(x_seq)\n",
    "\n",
    "plt.plot(x_seq, log_y)\n",
    "plt.title('Standard Logistic Function')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make this a bit more concrete with a simple example. Say we have a linear model:\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1\\ x$$\n",
    "\n",
    "Now, depending on the value of $\\hat{y}$ we want to classify the output from a logistic regression model as either `0` or `1`. We can use the linear model in the logistic function as follows:\n",
    "\n",
    "$$F(\\hat{y}) = \\frac{1}{1 + e^{-\\kappa(\\beta_0 + \\beta_1\\ x)}} $$\n",
    "\n",
    "In this way we transform the continious output of the linear model defined on $-\\infty \\le \\hat{y} \\le \\infty$ to a binary response, $0 \\le F(\\hat{y}) \\le 1$\n",
    "\n",
    "The aforementioned function maps the output of a linear model to the response values of the label. This function is known as the **link function**, since it links the linear response to the label values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Deviance?\n",
    "\n",
    "The significance of the GLM is expressed in terms of something called **deviance**. It can be a bit of a challenge to wrap your head around what deviance really means. It turns out that you already know one form of deviance. The familiar mean squared error of a model with Normally distributed residuals is the deviance of this model.  \n",
    "\n",
    "For the model above the residuals would be Binomially distributed. We need a more general formulation of deviance for such a model. For a model with label vector $y$, predictions$\\hat{\\mu}$, and estimated model parameters, $\\hat{\\theta}$ the general form of deviance can be written:\n",
    "\n",
    "\\begin{align}\n",
    "D(y,\\hat{\\mu}) &= 2 \\Big( log \\big( p(y | \\hat{\\theta}_{S}) \\big) - log \\big( p(y | \\hat{\\theta}_{0}) \\big) \\Big) \\\\\n",
    "&= 2 \\Big( \\mathcal{l}(y | \\hat{\\theta}_{S})  -  \\mathcal{l}(y | \\hat{\\theta}_{0})  \\Big)\n",
    "\\end{align}\n",
    "\n",
    "where,    \n",
    "- $\\mathcal{l}(y | \\hat{\\theta}) \\big)$ is the log likelihood of a model with estimated parameters, $\\hat{\\theta}$, given labels $y$.    \n",
    "- $\\hat{\\theta}_{S}$ are the estimated parameters of a **saturated**; a hypothetical model, $M_S$, with a parameter for each observation, and therefore having the best possible fit to the training data.   \n",
    "- $\\hat{\\theta}_{0}$ are the actual estimated parameters of the model, $M_0$, that we wish to evaluate.   \n",
    "\n",
    "This all seems rather abstract. In particular, what use is this hypothetical saturated model? \n",
    "\n",
    "Fortunately, we can use the **deviance ratio** to compare models in a form we can actually work with. The trick is to recognize that the log likelihood of the saturated model, $\\mathcal{l}(y | \\hat{\\theta}_{0})$, is just a constant. With this insight we can rewrite the deviance as:\n",
    "\n",
    "$$D(y,\\hat{\\mu}) = Constant + 2 \\mathcal{l}(y | \\hat{\\theta}_{0})$$\n",
    "\n",
    "This looks a bit better! Now we can work with the deviance ratio to compare two models, $M_1$ and $M_0$:   \n",
    "\n",
    "\\begin{align}\n",
    "D(\\hat{\\theta}_1,\\hat{\\theta}_0) &= \\frac{Constant + 2 \\mathcal{l}(y | \\hat{\\theta}_{0})}{Constant + 2 \\mathcal{l}(y | \\hat{\\theta}_{1})} \\\\ \n",
    "&= Constant + 2 \\mathcal{l}(y | \\hat{\\theta}_{0}) - Constant - 2 \\mathcal{l}(y | \\hat{\\theta}_{1}) \\\\\n",
    "&= 2 \\mathcal{l}(y | \\hat{\\theta}_{0}) - 2 \\mathcal{l}(y | \\hat{\\theta}_{1})\n",
    "\\end{align}\n",
    "\n",
    "Which model should we use as a comparison in the above relation? The **null model** is a good choice. The null model is just a model that makes predictions based on the expected value of mean. This formulation is called the **null deviance**, and is what is typically displayed by most statistical software packages, including statsmodels. Unfortunately, this quantity is usually just called, deviance, even when it is actually null deviance, something rather different.   \n",
    "\n",
    "A nice property of null deviance is that for large number of samples (approaching infinity), it is Chi Squared distributed. Thus, we can test the statistical significance of the model against the null model using the Chi Squared test. Since the derivation of this test depends on something called Wilk's theorem, this is often referred to as **Wilk's test**. \n",
    "\n",
    "The null deviance is also useful for comparing models. If you have two models and want to know which is better, pick the one with the maximum null deviance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Example\n",
    "\n",
    "To make these concepts concrete we will now build and test a classification model with Binomial response. To understand the features and labels of this model it will be useful to look at the model matrix and the label column. Execute the code in the cell below to display the head of the label array and the model matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'bad_credit ~ C(credit_history) + C(purpose) + C(gender_status) + C(time_employed_yrs) +C(other_credit_outstanding)\\\n",
    "           + C(property) + C(job_category) + loan_duration_mo + payment_pcnt_income + age_yrs'\n",
    "Y, X = dmatrices(formula, data=credit_data_train)\n",
    "print(Y[:5])\n",
    "print(X[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label array is binary with values $[0,1$. This response is must be Binomially distributed making this a nonlinear response problem.  \n",
    "\n",
    "The feature (design) matrix is not surprising. The first column contains the intercept term of all 1s. The categorical feature columns are all one-hot encoded. The last three columns are the normalized values of the numeric features. \n",
    "\n",
    "The statsmodels [GLM](https://www.statsmodels.org/stable/examples/notebooks/generated/glm.html) function implements the [generalize linear model](https://www.statsmodels.org/stable/glm.html#families) for a number of response distributions. The code in the cell below uses the Binomial family as the distribution of the label. Execute this code and examine the model summary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(4536)\n",
    "glm_model = smf.glm(formula, data=credit_data_train, family=sm.families.Binomial()).fit()\n",
    "glm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the summary of the model noticing the following:    \n",
    "- The distribution family of the label is Binomial as expected. \n",
    "- The **link function** is the logit. The link function transforms the results of the linear model to the label distribution. \n",
    "- The Pearson Chi Squared statistic shows that the model is significant compared to a null model based on the difference in deviance.   \n",
    "- The standard errors, confidence intervals and p-values show that most of the features are not significant. In other words, we cannot reject the null hypothesis that many of these coefficients have a value of 0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Probability to Class\n",
    "\n",
    "The predictions from a generalized linear model are the inverse link function transformed values of the linear model. This output or response in in the from of the probability distribution chosen. For our example, the response is Binomially distributed. The probability of the response (1) is computed using the **logit function** as the transformation.     \n",
    "\n",
    "In order to generate binary predictions (good or bad credit), we need to decide on a cutoff value for the probability. The cutoff value separates the two cases. Probabilities below the cutoff are classified as the negative outcome (good credit). Probabilities above the cutoff are classified as the positive outcome (bad credit). In effect, applying a cutoff in this manner is a **one-sided hypothesis test**; can you reject the null case and find a positive case?     \n",
    "\n",
    "Commonly, we choose 0.5 as a cutoff.  But  this choice is arbitrary and we can set it to whatever probability we choose, given the problem at hand. If we have a model where False Positives are costly, we might decide to increase the cutoff, and vice-versa if False Negatives are more costly. As an example, cost to the bank from the loss from issuing a loan to a bad credit customer might well exceed the cost of not issuing a loan to a good credit customer. The later cost might include the cost of loss of business and the customer service costs of correcting the status of otherwise good customers. \n",
    "\n",
    "The code in the cell below does the following:    \n",
    "1. Computes the probabilities of the response variables using the `predict` method.      \n",
    "2. The probabilities are transformed from probabilities to the classes using the cutoff value.   \n",
    "3. A column with the classes of the label are created from the binary values.   \n",
    "\n",
    "Execute the code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  \n",
    "\n",
    "threshold = 0.5\n",
    "def score_model(df, model, threshold):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    df.loc[:,'prediction_probs'] = model.predict(df)\n",
    "    df.loc[:,'predicted_credit'] = ['bad' if x > threshold else 'good' for x in df.loc[:,'prediction_probs']]\n",
    "    df.loc[:,'credit_actual'] = ['bad' if x==1 else 'good' for x in df.loc[:,'bad_credit']]\n",
    "    warnings.resetwarnings()\n",
    "    return df\n",
    "\n",
    "credit_data_train = score_model(credit_data_train, glm_model, threshold)\n",
    "credit_data_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the `predicted_credit` and `credit_actual` columns. Notice that the prediction agrees with the actual value in several cases. However, there are incorrect predictions. Thinking in terms of a hypothesis test:   \n",
    "1. **Type I errors** occur when a true negative case is classified as positive. The null hypothesis is incorrectly rejected.   \n",
    "2. **Type II errors** occur when a true positive case is classified as negative. There is a failure to reject the null hypothesis.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 26-2:** For the training data you can create a plot showing the cumulative probabilities computed for each of the classes as follows:   \n",
    "> 1. Use the `seaborn.ecdfplot` function (empirical CDF) with `hue='bad_credit'` to display the CDF of the `prediction_probs`. \n",
    "> 2. Use the matplotlib axis returned to add a vertical red dotted line to the plot at the threshold value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the plot you have created. The fractions of correct and incorrect classifications (decisions) can be read on the plot from the proportion above or below the crossing point between each CDF and the threshold. For example, the number of Type II errors is proportion of the positive curve to the left of the threshold. Given the threshold value, do you expect a large number of Type II errors and a smaller number of Type I errors, and why?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model   \n",
    "\n",
    "How can we evaluate the performance of a classifier model? As with any statistical or machine learning model there is no one metric which can be considered 'best'. We will consider only a few of the possibilities here.  \n",
    "\n",
    "First, let us consider all the possible outcomes of the hypothesis test. \n",
    "\n",
    " - **True Positives (TP):** We predicted bad credit and the credit is bad.\n",
    " - **True Negatives (TN):** We predicted good credit and the credit is good.\n",
    " - **False Positives (FP):** We predicted bad credit but the credit was good; a **Type I error**.\n",
    " - **False Negatives (FN):** We predicted good credit but the credit was bad; a **Type II error**.\n",
    " \n",
    "For the Binomial case, we can lay these options out in a **truth table**, which is commonly called a **confusion matrix**:\n",
    " \n",
    " |  | Classified Positive | Classified Negative |\n",
    "| ---- | :---: | :---: |\n",
    "|Positive | TP | FN |  \n",
    "|Negative | FP | TN |\n",
    "\n",
    "Some commonly used metrics are computed directly from the foregoing quantities:\n",
    "\n",
    "- **Accuracy** = (TP + TN) / (TP + FP + TN + FN)\n",
    "- **Precision** = TP / (TP + FP)\n",
    "   - Precision is the correct fraction of the positive relevant class predictions.\n",
    "- **Recall** = TP / (TP + FN)\n",
    "   - Recall is the correct fraction of the relevant positive class members predicted.\n",
    "- **F1** = $\\frac{2\\ precision\\ recall}{precision + recall}$ = $\\frac{TP}{TP + 1/2(FP +FN)}$      \n",
    "   - Is the **harmonic mean** between precision and recall. In other words, a statistic which trades-offs precision and recall. \n",
    "   \n",
    "These metrics can be computed using the [sklearn.metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) package. In this case we use some of the classification metrics. Now, execute the code in the cell below to display performance metrics for the classifier model using the training data.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(df):\n",
    "    labels = df.loc[:,'credit_actual']\n",
    "    scores = df.loc[:,'predicted_credit']\n",
    "    metrics = sklm.precision_recall_fscore_support(labels, scores)\n",
    "    conf = sklm.confusion_matrix(labels, scores)\n",
    "    print('                 Confusion matrix')\n",
    "    print('                 Score positive    Score negative')\n",
    "    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n",
    "    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n",
    "    print('')\n",
    "    print('Accuracy  %0.2f' % sklm.accuracy_score(labels, scores))\n",
    "    print(' ')\n",
    "    print('           Positive      Negative')\n",
    "    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n",
    "    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n",
    "    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n",
    "    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])\n",
    "    \n",
    "print_metrics(credit_data_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these metrics which can be interpreted as follows:   \n",
    "1. A perfect classifier would produce a confusion matrix with entries only on the diagonal; no errors. In this case, there are significant errors. The number of false positives compared to the total number of positives is relatively low. However, fewer that half of the positive cases have been correctly classified.    \n",
    "2. The overall accuracy of the model is reasonable.  \n",
    "3. However, the recall statistic shows a significant problem with the model. The positive case recall is only 0.40, consistent with the low fraction of the correctly classified positive cases.   \n",
    "4. Likewise, the precision is lower for positive cases as well.     \n",
    "\n",
    "In summary, the initial classifier model is doing a poor job of detecting bad credit cases. In part, this performance results from the imbalance between the number of positive and negative cases in the data set.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation of the model based on training data is undoubtedly biased, since this data was used to fit the model. Using the **held-back test data** is a more objective approach to model evaluation. Execute the code in the cell below to evaluated the model using the test data.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_data_test = score_model(credit_data_test, glm_model, threshold)\n",
    "print_metrics(credit_data_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, these results are consistent with those obtained using the training data. However, the results obtained with the test data are a bit worse than for the training data. This is expected, since the test data are a random sample of the original data.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding a better threshold\n",
    "\n",
    "The forgoing results are not good. Since the choice of threshold was arbitrary, it is likely that another choice could yield better results. A hyperparameter search could find a threshold producing a better result.    \n",
    "\n",
    "The question is, which metric should be used to optimize model performance? Accuracy, can be misleading, particularly in cases with imbalanced label values. For this problem we will assume that the bank will be more concerned with identifying potential bad credit customers. These considerations make recall seem to be a reasonable metric. In this case we will use F1, since it trades-off precision and recall.        \n",
    "\n",
    "> **Exercise 26-3:** The code in the cell below performs a hyperparameter search of the cutoff threshold measured using the F1 score for the positive cases. Execute this code and examine the chart displayed.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def threshold_test(model, df_train, df_test, thresholds, formula=formula):\n",
    "    '''Function that tests accuracy of classifier for different threshold values'''\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "    for i,threshold in enumerate(thresholds):\n",
    "        ## Compute the training accuracy\n",
    "        df_train = score_model(df_train, model, threshold)\n",
    "        accuracy_train.append(sklm.precision_recall_fscore_support(df_test.loc[:,'credit_actual'], df_test.loc[:,'predicted_credit'])[2][0])\n",
    "        ## Then compute the test accuracy\n",
    "        df_test = score_model(df_test, model, threshold)\n",
    "        accuracy_test.append(sklm.precision_recall_fscore_support(df_test.loc[:,'credit_actual'], df_test.loc[:,'predicted_credit'])[2][0])      \n",
    "    return accuracy_train, accuracy_test\n",
    "\n",
    "\n",
    "def plot_thresholds(thresholds, accuracy_train, accuracy_test):\n",
    "    fig, ax = plt.subplots(figsize=(8, 5)) # define axis  \n",
    "    ax.plot(thresholds, accuracy_train, label='Training F1')\n",
    "    ax.plot(thresholds, accuracy_test, label='Test F1')\n",
    "    ax.set_ylabel('F1')\n",
    "    ax.set_xlabel('Threshold value')\n",
    "    ax.set_title('F1 vs. decision threshold')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "thresholds = np.arange(0.2, 0.8, step = 0.05)\n",
    "accuracy_train, accuracy_test = threshold_test(glm_model, credit_data_train, credit_data_test, thresholds)\n",
    "plot_thresholds(thresholds, accuracy_train, accuracy_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine your plot. What does this downward slope of the F1 curve with threshold value tell you about the trade-off between F1 and threshold? How is this trade-off dependent on the cost of each type of error for the problem at hand?    \n",
    "> **End of exercise.**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 26-4:** You will now test how the model evaluation changes when using a threshold value to 0.35. While this choice is a bit arbitrary, the goal is to improve the performance at detecting positive cases by reducing Type II errors at the expense of Type I errors with negative cases. Do the following:   \n",
    "> 1. In the first cell below set the threshold value, then score the model and print the metrics using the training data.  \n",
    "> 2. In the second cell below use the new threshold value to score the model and print the metrics using the test data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Your code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare the performance metrics for the training and testing data with the original threshold value of 0.5 with those of the updated threshold of 0.35.   \n",
    "> 1. How have the errors shown in the confusion matrices changed?   \n",
    "> 2. How have the recall and precision changed between the positive and negative cases and what does this tell you about the trade-off between Type I and Type II errors?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the testing data the plot below shows the probabilities computed for each of the classes using the testing data. This plot is the same type you created in Exercise 26-2.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.ecdfplot(x='prediction_probs', hue='bad_credit', data=credit_data_train) \n",
    "ax.axvline(0.35, color='red', linestyle='dotted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this plot to the one created in Exercise 26-2. Notice how the number of Type II errors arising from the positive case curve will be lower. This improvement is at the expense of Type I errors arising from the negative curve.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying regularization      \n",
    "\n",
    "So far, we have been working with an over-fit model. Now we will explore the application of regularization to the model. The code in the cell below computes the training and test F1 statistics of the model as a function of the elastic net regularization parameter $\\alpha$. The training and test F1 statistics are then plotted as a function of $\\alpha$. Execute this code to execute the computation and display the plot.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def regularized_coefs(df_train, df_test, alphas, n_coefs=31,\n",
    "                      formula = formula, threshold=threshold):\n",
    "    '''Function that computes a linear model for each value of the regualarization \n",
    "    parameter alpha and returns an array of the coefficient values. The L1_wt \n",
    "    determines the trade-off between L1 and L2 regualarization'''\n",
    "    coefs = np.zeros((len(alphas),n_coefs + 1))\n",
    "    accuracy_train = []\n",
    "    accuracy_test = []\n",
    "    for i,alpha in enumerate(alphas):\n",
    "        ## First compute the training MSE\n",
    "        #### Complete the line of code below\n",
    "        temp_mod = smf.glm(formula, data=df_train).fit_regularized(method='elastic_net', alpha=alpha)\n",
    "        ## Compute the training accuracy\n",
    "        df_train = score_model(df_train, temp_mod, threshold)\n",
    "        coefs[i,:] = temp_mod.params\n",
    "        # accuracy_train.append(sklm.accuracy_score(df_test.loc[:,'credit_actual'], df_test.loc[:,'predicted_credit']))\n",
    "        accuracy_train.append(sklm.precision_recall_fscore_support(df_test.loc[:,'credit_actual'], df_test.loc[:,'predicted_credit'])[2][0])\n",
    "        ## Then compute the test accuracy\n",
    "        df_test = score_model(df_test, temp_mod, threshold)\n",
    "#        accuracy_test.append(sklm.accuracy_score(df_test.loc[:,'credit_actual'], df_test.loc[:,'predicted_credit'])) \n",
    "        accuracy_test.append(sklm.precision_recall_fscore_support(df_test.loc[:,'credit_actual'], df_test.loc[:,'predicted_credit'])[2][0])      \n",
    "    return coefs, accuracy_train, accuracy_test\n",
    "\n",
    "\n",
    "def plot_coefs(coefs, alphas, accuracy_train, accuracy_test, ylim=None):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12, 5)) # define axis\n",
    "    for i in range(coefs.shape[1]): # Iterate over coefficients\n",
    "        ax[0].plot(alphas, coefs[:,i])\n",
    "    ax[0].axhline(0.0, color='red', linestyle='--', linewidth=0.5)\n",
    "    ax[0].set_ylabel('Partial slope values')\n",
    "    ax[0].set_xlabel('alpha')\n",
    "    ax[0].set_title('Parial slope values vs. regularization parameter number')\n",
    "    if ylim is not None: ax[0].set_ylim(ylim)\n",
    "    \n",
    "    ax[1].plot(alphas, accuracy_train, label='Training F1')\n",
    "    ax[1].plot(alphas, accuracy_test, label='Test F1')\n",
    "    ax[1].set_ylabel('F1')\n",
    "    ax[1].set_xlabel('alpha')\n",
    "    ax[1].set_title('F1 vs. regularization parameter')\n",
    "    plt.legend()\n",
    "    plt.show()   \n",
    "    \n",
    "threshold = 0.35\n",
    "alphas = np.arange(0.0, 0.002, step = 0.00005)\n",
    "#alphas = np.arange(0.0, 0.05, step = 0.0005)\n",
    "Betas, accuracy_train, accuracy_test = regularized_coefs(credit_data_train, credit_data_test, alphas)\n",
    "plot_coefs(Betas, alphas, accuracy_train, accuracy_test, ylim=[-0.3,0.5])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are a bit erratic looking, but consistent with what is expected. The model coefficients can been seen being constrained and in some cases driven toward 0 by the elastic net regularization as the hyperparameter increases. The curve of the training and test errors is rather jagged. However, one can see that a reasonable choice of hyperparameter is around 0.0007.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 26-5:** Using the regularization parameter identified and the threshold value of 0.35 you will now evaluate the model. In the cells below you will create and execute code to evaluate the model using first the training data and then the testing data by these steps:   \n",
    "> 1. Fit the glm model using elastic net regularization and $\\alpha = 0.0007$. You should fit the model once with the training data.   \n",
    "> 2. Score the model using $threshold = 0.35$ using either the training of testing data.   \n",
    "> 3. Compute and print the model evaluation metrics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Your code goes here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The goal of regularization is not to improve model training performance, but rather to improve generalization performance. Keeping the foregoing in mind, compare the performance metrics for the training and testing data of the OLS model with threshold value of 0.35 with those of the regularized model with the same threshold value.   \n",
    "> 1. Have the errors shown in the confusion matrices changed for either the training of testing data in a way that would indicate model performance is significantly reduced?   \n",
    "> 2. How have the recall and precision changed between the positive and negative cases and what does this change tell you about the Type I and Type II generalization errors when regularization is applied?\n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 26-6:** as a final step in comparing the coefficients of the OLS and regularized models. Execute the code in the cell below to display the coefficient values for the regularized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_model_regularized.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compare these coefficient values to the ones displayed by the summary method for the OLS model.    \n",
    "> 1. How well are the few very large coefficient values computed for the OLS model constrained by the regularization? Pay particular attention to the coefficients of the numeric variables and at least one categorical dummy variable.  \n",
    "> 2. Are some of the coefficients for both numeric and categorical dummy variables pushed to exactly zero, and what does this tell you about model selection? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Copyright 2017, 2018, 2019, 2020, 2021 Stephen F Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
