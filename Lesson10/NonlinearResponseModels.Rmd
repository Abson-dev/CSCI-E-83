---
title: "Models with Nonlinear Response"
author: "Steve Elston"
date: "03/28/2021"
output:
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("/usr/bin/python3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```

## Introduction  

- Models with nonlinear response   

- The generalized linear model     

- Link function transforms to linear model  


## Models with Nonlinear Response    

How do we deal with models that do not have nonlinear response variables?   

- Example: binary response variable, $[0,1]$ response    
   - A binary classifier      
   
- Intensity of an arrival process, $poisson(\lambda)$ response   

- The **generalized linear model (GLM)** is a framework for these models   
   
- For each distribution use a **link function** to transform to a linear model   
   - Find coefficients of transformed model with OLS!
   - Works for all exponential family response distributions     
   
   
## The Generalized Linear Model   

Start with a regression model for binary classification - **logistic regression**   

- Use Binomial distribution to model $y$ successes in $n$ trials    

$$y \sim Bi(n, \theta)$$

- But finding best fit value of parameter $\theta$ directly is a nonlinear problem!   
- Instead, define a **link function**, known as the **logit function** for parameter $\lambda$     

$$\lambda = log \Bigg\{ \frac{\theta}{1-\theta} \Bigg\}$$

- After a bit of algebra we find the exponential relationship with $y$

$$y = n\ log(1+ e^\lambda)$$


## The Generalized Linear Model  

Formulate a linear regression problem using the link function      

$$\lambda = log \Bigg\{ \frac{\theta}{1-\theta} \Bigg\} = X \vec{b}$$    
for model matrix $X$ and coefficient vector $\vec{b}$

- Given estimate of $\lambda$, compute $\theta$    

$$\theta = \frac{1}{1 + e^\lambda} = \frac{1}{1 + e^{X \vec{b}}}$$


## The Generalized Linear Model  

Consider the example for a single feature, $x$     

- The linear model has only slope and intercept, $[ \beta_0, \beta_1]$ 

$$\lambda_i = log \Bigg\{ \frac{\theta_i}{1-\theta_i} \Bigg\} = \beta_0 + \beta_1 x_i$$      

and    

$$\theta_i = \frac{1}{1 + e^{\beta_0 + \beta_1 x_i}}$$


## Logisitc Regaresion Model

What does the transformation function look like?    

$$y = \frac{1}{1 + e^\lambda}$$

```{python, echo=FALSE}
import pandas as pd
import numpy as np
import numpy.random as nr
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import statsmodels.formula.api as smf
import scipy.stats as ss  
from math import atan

test_scores = pd.read_csv('../data/hsb2.csv', index_col=0)
#test_scores.head(10)
```


```{python, echo=FALSE}
# Plot the logistic transformation function (f(x) above)
x_seq = np.linspace(-7, 7, 100)

def log_fun(x, center=0, scale=1):
    e = np.exp(-scale*(x-center))
    log_out = 1./(1. + e)
    return log_out

log_fun_vectorized = np.vectorize(log_fun)

log_y = log_fun_vectorized(x_seq)

plt.plot(x_seq, log_y)
plt.title('Standard Logistic Function')
plt.xlabel('x')
plt.ylabel('y')
plt.grid()
plt.show()
```

- The response is bound in the range $[0,1]$

- We say the logistic transformation **squashes** the linear response $\lambda =X \vec{b}$ to binary, $[0,1]$   

- Can set a **decision threshold** for binary response    
   - Default $= 0.5$


## Evaluation of Classifiers   

How can we evaluate a classifier's accuracy? 

- Determine proportions of test cases which are classified as:     
   - True Positives (TP): Are positive and should be positive      
   - True Negatives (TN): Are negative and should be negative    
   - False Positives (FP): Classified as positive but are actually negative   
   - False Negatives (FN): Classified as negative but are actually positive   

- Organize these metrics into a **confusion matrix**      

|  | Classified Positive | Classified Negative |
| ---- | :---: | :---: |
|Positive | TP | FN |  
|Negative | FP | TN |



## Evaluation of Classifiers   


The other metrics are defined as follows:

- Accuracy = (TP + TN) / (TP + FP + TN + FN)      

- Selectivity or Precision = TP / (TP + FP)       
   - Precision is the fraction of the relevant class predictions are actually correct     
   
- Sensitivity or Recall = TP / (TP + FN)
   - Recall is the fraction of the relevant class were we able to predict    


- Is a trade-off between precision and recall     
   - Consider changing the decision threshold    
   - High threshold $\rightarrow$ higher recall, more false negatives   
   - Low threshold $\rightarrow$ higher precision, more false postives    

## Example of Logisitic Regression    

How well can we predict the type of school given the test scores?     

```{python}
## Prep the data
test_scores['schtyp'] = np.subtract(test_scores['schtyp'], 1)
for col in ['read', 'write', 'math', 'science', 'socst']:
    test_scores[col] = np.divide(np.subtract(test_scores[col], np.mean(test_scores[col])), np.std(test_scores[col]))

## Fit the model
formula = 'schtyp ~ math'
logistic_reg_model = smf.glm(formula=formula, data=test_scores, family=sm.families.Binomial()).fit()

## score the results 
threshold=0.18
test_scores['predicted'] = logistic_reg_model.predict()
test_scores['score'] = [1 if x>threshold else 0 for x in test_scores['predicted']]

print(logistic_reg_model.summary())
```


## Example of Logisitic Regression    

The data frame now looks like this with the predicted probability and the binary scores:

```{python}
test_scores.head(20)
```


## Example of Logisitic Regression    

Now, evaluate the model - the classifier is almost useless - **no Kagle awards!**:   

```{python}
import sklearn.metrics as sklm  
def print_metrics(labels, scores):
    metrics = sklm.precision_recall_fscore_support(labels, scores)
    conf = sklm.confusion_matrix(labels, scores)
    print('                 Confusion matrix')
    print('                 Score positive    Score negative')
    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])
    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])
    print('')
    print('Accuracy  %0.2f' % sklm.accuracy_score(labels, scores))
    print(' ')
    print('           Positive      Negative')
    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])
    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])
    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])
    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])
    
print_metrics(test_scores['schtyp'], test_scores['score'])    
```

