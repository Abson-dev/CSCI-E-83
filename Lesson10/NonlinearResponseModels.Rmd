---
title: "Models with Nonlinear Response"
author: "Steve Elston"
date: "03/28/2021"
output:
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("/usr/bin/python3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```

## Introduction  

- Models with nonlinear response   

- The generalized linear model     

- Link function transforms to linear model  


## Models with Nonlinear Response    

How do we deal with models that do not have nonlinear response variables?   

- Example: binary response variable, $[0,1]$, $Bin(\theta)$ distributed     
   - Probability parameter $\theta$
   - A binary classifier      
   
- Intensity of an arrival process, $poisson(\lambda)$ response   
   - $\lambda$ 
   - Estimate the parameter $\lambda$    

- The **generalized linear model (GLM)** is a framework for these models   
   
- For each distribution use a **link function** to transform to a linear model   
   - Find coefficients of transformed model with OLS!
   - Works for all exponential family response distributions     
   
   
## The Generalized Linear Model   

Start with a regression model for binary classification - **logistic regression**   

- Use Binomial distribution to model $y$ successes in $n$ trials    

$$y \sim Bi(n, \theta)$$

- But finding best fit value of parameter $\theta$ directly is a nonlinear problem!   
- Instead, define a **link function**, known as the **logit function** for parameter $\lambda$     

$$\lambda = log \Bigg\{ \frac{\theta}{1-\theta} \Bigg\}$$

- After a bit of algebra we find the exponential relationship with $y$

$$y = n\ log(1+ e^\lambda)$$


## The Generalized Linear Model  

Formulate a linear regression problem using the link function      

$$\lambda = log \Bigg\{ \frac{\theta}{1-\theta} \Bigg\} = X \vec{b}$$    
for model matrix $X$ and coefficient vector $\vec{b}$

- Given estimate of $\lambda$, compute $\theta$    

$$\theta = \frac{1}{1 + e^\lambda} = \frac{1}{1 + e^{X \vec{b}}}$$


## The Generalized Linear Model  

Consider the example for a single feature, $x$     

- The linear model has only slope and intercept, $[ \beta_0, \beta_1]$ 

$$\lambda_i = log \Bigg\{ \frac{\theta_i}{1-\theta_i} \Bigg\} = \beta_0 + \beta_1 x_i$$      

and    

$$\theta_i = \frac{1}{1 + e^{\beta_0 + \beta_1 x_i}}$$


## Logisitc Regaresion Model

What does the transformation function look like?    

$$y = \frac{1}{1 + e^\lambda}$$

```{python, echo=FALSE}
import pandas as pd
import numpy as np
import numpy.random as nr
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import statsmodels.formula.api as smf
import scipy.stats as ss  
from math import atan

test_scores = pd.read_csv('../data/hsb2.csv', index_col=0)
#test_scores.head(10)
```


```{python, echo=FALSE}
# Plot the logistic transformation function (f(x) above)
x_seq = np.linspace(-7, 7, 100)

def log_fun(x, center=0, scale=1):
    e = np.exp(-scale*(x-center))
    log_out = 1./(1. + e)
    return log_out

log_fun_vectorized = np.vectorize(log_fun)

log_y = log_fun_vectorized(x_seq)

plt.plot(x_seq, log_y)
plt.title('Standard Logistic Function')
plt.xlabel('x')
plt.ylabel('y')
plt.grid()
plt.show()
```

- The response is bound in the range $[0,1]$

- We say the logistic transformation **squashes** the linear response $\lambda =X \vec{b}$ to binary, $[0,1]$   

- Can set a **decision threshold** for binary response    
   - Default $= 0.5$


## Evaluation of Classifiers   

How can we evaluate a classifier's accuracy? 

- Determine proportions of test cases which are classified as:     
   - True Positives (TP): Are positive and should be positive      
   - True Negatives (TN): Are negative and should be negative    
   - False Positives (FP): Classified as positive but are actually negative   
   - False Negatives (FN): Classified as negative but are actually positive   

- Organize these metrics into a **confusion matrix**      

|  | Classified Positive | Classified Negative |
| ---- | :---: | :---: |
|Positive | TP | FN |  
|Negative | FP | TN |



## Evaluation of Classifiers   


The other metrics are defined as follows:

- Accuracy = (TP + TN) / (TP + FP + TN + FN)      

- Selectivity or Precision = TP / (TP + FP)       
   - Precision is the fraction of the relevant class predictions are actually correct     
   
- Sensitivity or Recall = TP / (TP + FN)
   - Recall is the fraction of the relevant class were we able to predict    


- Is a trade-off between precision and recall     
   - Consider changing the decision threshold    
   - High threshold $\rightarrow$ higher recall, more false negatives   
   - Low threshold $\rightarrow$ higher precision, more false postives    

## Example of Logisitic Regression    

How well can we predict the type of school given the test scores?     

```{python}
## Prep the data
test_scores['schtyp'] = np.subtract(test_scores['schtyp'], 1)
for col in ['read', 'write', 'math', 'science', 'socst']:
    test_scores[col] = np.divide(np.subtract(test_scores[col], np.mean(test_scores[col])), np.std(test_scores[col]))

## Fit the model
formula = 'schtyp ~ math'
logistic_reg_model = smf.glm(formula=formula, data=test_scores, family=sm.families.Binomial()).fit()

## score the results 
threshold=0.18
test_scores['predicted'] = logistic_reg_model.predict()
test_scores['score'] = [1 if x>threshold else 0 for x in test_scores['predicted']]

print(logistic_reg_model.summary())
```


## Example of Logisitic Regression    

The data frame now looks like this with the predicted probability and the binary scores:

```{python}
test_scores.head(20)
```


## Example of Logisitic Regression    

Now, evaluate the model - the classifier is almost useless - **no Kagle awards!**:   

```{python}
import sklearn.metrics as sklm  
def print_metrics(labels, scores):
    metrics = sklm.precision_recall_fscore_support(labels, scores)
    conf = sklm.confusion_matrix(labels, scores)
    print('                 Confusion matrix')
    print('                 Score positive    Score negative')
    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])
    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])
    print('')
    print('Accuracy  %0.2f' % sklm.accuracy_score(labels, scores))
    print(' ')
    print('           Positive      Negative')
    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])
    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])
    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])
    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])
    
print_metrics(test_scores['schtyp'], test_scores['score'])    
```


## What is Deviance?

The significance of a GLM is expressed in terms of **deviance**     

- Deviance can be challenging to understand what deviance really means      

- Formally, deviance is the expected value of the ratio of one probability densitt function, $f_1(y)$ to a second probability density function, $f_2(y)$:

$$D\big[f_1(y),f_1(y) \Big] = 2 \int_y f_1(y) log \Bigg\{ \frac{f_1(y)}{f_2(y)} \Bigg\} dy$$

If $f_1(y) \rightarrow f_1(y)$, then:

$$log \Bigg\{
\frac{f_1(y)}{f_2(y)} \Bigg\} \rightarrow log(1) \rightarrow 0$$

And, $D\big[f_1(y),f_1(y) \Big] \rightarrow 0$


## What is Deviance? 

Deviance is a measure of the divergence between two distributions   

- Consider an easy to understand form of deviance for the Normal distribution    
   - the mean squared error of a model is the deviance  



## What is Deviance?

For the model above the residuals would be Binomially distributed. We need a more general formulation of deviance for such a model. For a model with label vector $y$, predictions$\hat{\mu}$, and estimated model parameters, $\hat{\theta}$ the general form of deviance can be written:

\begin{align}
D(y,\hat{\mu}) &= 2 \Big( log \big( p(y | \hat{\theta}_{S}) \big) - log \big( p(y | \hat{\theta}_{0}) \big) \Big) \\
&= 2 \Big( \mathcal{l}(y | \hat{\theta}_{S})  -  \mathcal{l}(y | \hat{\theta}_{0})  \Big)
\end{align}

where,    
- $\mathcal{l}(y | \hat{\theta}) \big)$ is the log likelihood of a model with estimated parameters, $\hat{\theta}$, given labels $y$.    
- $\hat{\theta}_{S}$ are the estimated parameters of a **saturated**; a hypothetical model, $M_S$, with a parameter for each observation, and therefore having the best possible fit to the training data.   
- $\hat{\theta}_{0}$ are the actual estimated parameters of the model, $M_0$, that we wish to evaluate.   



## What is Deviance?


This all seems rather abstract. In particular, what use is this hypothetical saturated model? 

Fortunately, we can use the **deviance ratio** to compare models in a form we can actually work with. The trick is to recognize that the log likelihood of the saturated model, $\mathcal{l}(y | \hat{\theta}_{0})$, is just a constant. With this insight we can rewrite the deviance as:

$$D(y,\hat{\mu}) = Constant + 2 \mathcal{l}(y | \hat{\theta}_{0})$$

This looks a bit better! Now we can work with the deviance ratio to compare two models, $M_1$ and $M_0$:   

\begin{align}
D(\hat{\theta}_1,\hat{\theta}_0) &= \frac{Constant + 2 \mathcal{l}(y | \hat{\theta}_{0})}{Constant + 2 \mathcal{l}(y | \hat{\theta}_{1})} \\ 
&= Constant + 2 \mathcal{l}(y | \hat{\theta}_{0}) - Constant - 2 \mathcal{l}(y | \hat{\theta}_{1}) \\
&= 2 \mathcal{l}(y | \hat{\theta}_{0}) - 2 \mathcal{l}(y | \hat{\theta}_{1})
\end{align}



## What is Deviance?

Which model should we use as a comparison in the above relation? The **null model** is a good choice. The null model is just a model that makes predictions based on the expected value of mean. This formulation is called the **null deviance**, and is what is typically displayed by most statistical software packages, including statsmodels. Unfortunately, this quantity is usually just called, deviance, even when it is actually null deviance, something rather different.   

A nice property of null deviance is that for large number of samples (approaching infinity), it is Chi Squared distributed. Thus, we can test the statistical significance of the model against the null model using the Chi Squared test. Since the derivation of this test depends on something called Wilk's theorem, this is often referred to as **Wilk's test**. 

The null deviance is also useful for comparing models. If you have two models and want to know which is better, pick the one with the maximum null deviance.  
