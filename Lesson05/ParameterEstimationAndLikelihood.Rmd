
---
title: "Parameter Estimation and Likelihood"
author: "Steve Elston"
date: "01/13/2021"
output:
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("/usr/bin/python3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```

---------------------------------

## Introduction   

The concept of **likelihood** and **maximum likelihood estimation (MLE)** have been at the core of much of statistical modeling for about 100 years   

- In 21st Century, likelihood and MLE ideas continue to be foundational      

-  Understanding the concept of likelihood and the use of MLE methods is key to understanding many parametric statistical methods     
  - Likelihood is a measure of how well a model fits data     
  - MLE is a generic methods for parameter estimation    

- MLE used widely for machine learning models, including some deep learning models      


------------------------------------------------------

## Likelihood   

Likelihood is a measure of how well a **parametric model** fits a data sample

- For a data sample, $\mathbf{X} = x_1, x_2, \ldots, x_n$    

- Model for these data, with a parameter vector $\vec{\theta}$, is the conditional density function given the observations $\mathbf{X}$: 

$$\mathcal{L}(\vec{\theta}\ |\ \mathbf{X} ) = f(\mathbf{X}\ |\ \vec{\theta})$$

- $f(\mathbf{X}\ |\ \vec{\theta}$ can be either a PDF, for continuous distributions, or a PMF, for discrete distributions



------------------------------------------------------

## Likelihood   

In most practical cases, we work with the **log likelibood**      

- For observations, $\mathbf{X} = x_1, x_2, \ldots, x_n$, the log likelihood:    

$$l(\vec{\theta}\ |\ \mathbf{X}) = log\big( \mathcal{L}(\vec{\theta}\ |\ \mathbf{X}) \big) = \sum_{j} log \Big( f(x_j\ |\ \vec{\theta}) \Big)$$

- The log likelihood summed over the sample is much easier to work with       


-----------------------------------------

## Example: The Normal likelihood

The univariate Normal probability density function with parameter vector $(\mu,\sigma)$ for a single observation, $x$:     

$$f(x,\mu,\sigma^2) = -\frac{1}{(2 \pi \sigma^2)^{1/2}} exp\Big[ - \frac{1}{2 \sigma^2}  (x - \mu)^2 \Big]$$


For n observations, $\mathbf{X} = x_1, x_2, \ldots, x_n$, the log-likelihood:  

$$l(\mu, \sigma\ |\ \mathbf{X}) = - \frac{n}{2} log( 2 \pi \sigma^2 ) - \frac{1}{2 \sigma^2} \sum_{j=1}^n (x_j - \mu)^2$$
- The log-likelihood is a function of the parameters, $(\mu,\sigma)$$     



------------------------------------------

## Example: The Normal likelihood

An example to illustrate the foregoing concepts    

- Plot the likelihood for 5, 10 and 20 samples from a standard Normal distribution   

- Vary the parameter $\mu$, and assume the parameter $\sigma$ is fixed and known. The steps are:     
- A random sample is drawn from a standard Normal distribution    

- For the random sample the log-likelihood is computed at each location parameter value    

- Notice that as the number of observations increases so does the likelihood.


```{python, echo=FALSE}
## Imports
import matplotlib.pyplot as plt
from scipy.stats import norm
import numpy as np
import numpy.random as nr

def plot_likelihood_1(sample_dist, pdf, num_samples, start, stop, linestyle):
    ## Setup for plot
    plt.rc('font', size=24)
    fig, ax = plt.subplots(figsize=(5, 4), ) 
    X = np.arange(start, stop, step=0.05)

    ## Loop over number of samples
    for i,samps in enumerate(num_samples): 
        ## Compute a sample from standard Normal
        sample = sample_dist(samps)
        ## Loop over the x values and compute the likelihood
        y=[]
        for mu in X:
            y.append(pdf(sample, mu).sum())
        ## Plot the likelihood    
        _=ax.plot(X, y, linewidth=4, label= str(samps) + ' samples', linestyle=linestyle[i] )

    ## Add annotions to plot
    ax.vlines(0.0, ymin=min(y), ymax=0.0, linewidth=3, linestyles='dotted')
    ax.set_ylabel('Log-likelihood', fontsize=10)
    ax.set_xlabel('x', fontsize=10)
    ax.set_title('Normal log-likelihood for different sample sizes', fontsize=10)
    ax.legend(fontsize=10)
#    ax.set_xticklabels(ticks, fontsize= 10)
#    ax.set_yticklabels(ticks, fontsize= 10)
    plt.show()         


sample_dist = lambda x: nr.normal(size=x)
pdf = lambda x, y: norm.logpdf(x, loc=y)
num_samples = [5, 10, 20]
start = -10.0
stop = 10.0
linestyle = ['solid','dashed','dashdot']
plot_likelihood_1(sample_dist, pdf, num_samples, start, stop, linestyle)
```

-----------------------------------

## Example: Binomial Likelihood   

Example of log-likelihood for the Binomial distribution   

-Differences include with Normal distribution:   
  - Binomial distribution models **discrete events**      
  - Range of the single parameter, $\pi$, restricted to the range $0 \le \pi \le 1$      

- Binomial distribution has the following probability mass function (PMF) for $k$ successes in $n$ trials:        

$$
f(k, n, \pi) = \binom{n}{y} \pi^k (1 - \pi)^{n-k}
$$

- Log-likelihood is easily found:    

$$l(k, n | \pi) = log \binom{n}{k} + k\ log(\pi) + (n-k)\ log(1-\pi)$$

- Binomial log-likelihood has a strong dependence on both the sample size, $n$ and the number of successes, $k$         


-------------------------------------------------------------

## The Maximum Likehihood Estimator    

**Maximum likelihood estimator (MLE)** is a foundational tool for much of statistical inference and machine learning      

- Given a log-likelihood function, find the model parameters which maximize it  

- Further, knowing the distribution allows us to quantify the uncertainty of the MLE parameter estimates   

- The model parameter estimates found by MLE is Normal for large samples, a remarkable property 

- The MLE is a **point estimator**    

- An estimate of a single parameter value, or point value, with the highest likelihood  

-------------------------------------------------------------

## The Maximum Likehihood Estimator  

The maximum likelihood for the model parameters is achieved when two conditions are met:  

$$
\frac{\partial\ l(\theta)| \mathbf{X}}{\partial \theta} = 0 \\
\frac{\partial^2\ l(\theta)| \mathbf{X} }{\partial \theta^2} < 0
$$


Interpret these two conditions:     

- First derivative of log-likelihood function, or slope, is 0 at either maximum or minimum points    
  - In general, $\vec{\theta}$ is a vector of model parameters   
  - Partial derivatives of log-likelihood are a vector - the gradient with respect to the model parameters   
  - Gradient of the log-likelihood are known as the **score function**   
  
- The second derivatives of the log-likelihood indicates the curvature      
  - Maximum has negative curvature   
  - Minimum has positive curvature
  
  
-----------------------------------------------------

## Fisher information and properties of MLE     

The maximum likelihood estimator has useful and desireable properties  

- Start with a matrix of second partial derivatives of the log-likelihood function   

- Matrix is the **observed information matrix** of the model, $\mathcal{J}(\vec{\theta})$.   


$$
\mathcal{J}(\vec{\theta}) = 
\begin{pmatrix}
  \frac{\partial^2 l(\vec{\theta}\ |\ \mathbf{X})}{\partial \theta_1^2} & 
  \frac{\partial^2 l(\vec{\theta}\ |\ \mathbf{X})}{\partial \theta_2\ \partial \theta_1} & 
  \cdots & 
  \frac{\partial^2 l(\vec{\theta}\ |\ \mathbf{X})}{\partial \theta_n\ \partial \theta_1}\\
   \frac{\partial^2 l(\vec{\theta}\ |\ \mathbf{X})}{\partial \theta_1\ \partial \theta_2} &
   \frac{\partial^2 l(\vec{\theta}\ |\ \mathbf{X})}{\partial \theta_2^2} & 
   \cdots &
   \frac{\partial^2 l(\vec{\theta}\ |\ \mathbf{X})}{\partial \theta_2\ \partial \theta_n}\\
   \vdots & \vdots & \vdots & \vdots \\
   \frac{\partial^2 l(\vec{\theta}\ |\ \mathbf{X})}{\partial \theta_n\ \partial \theta_1} &
   \frac{\partial^2 l(\vec{\theta}\ |\ \mathbf{X})}{\partial \theta_n\ \partial \theta_2} & 
   \cdots &
   \frac{\partial^2 l(\vec{\theta}\ |\ \mathbf{X})}{\partial \theta_n^2} 
 \end{pmatrix}
$$

Useful properties of the information matrix   

- The more negative the values of the second partial derivatives, the greater the curvature of the log-likehihood    
  - log-likelihood likelihood function with more negative values has limbs    
  - Narow peak implies that the information on parameter values is high    
  
- The matrix is symmetric, or information is symmetric around the maximum likelihood point


-----------------------------------------------------

## Fisher information and properties of MLE   

Can one consider the information of the MLE before sampling data or performing an experiment? T

- Can use the **expected information** or **Fisher information**    

- Fisher information is the expectation over the second derivative of the observed information            
$$\mathcal{J}(\vec{\theta}) = -\mathbf{E}\big\{ \mathcal{I}(\vec{\theta}) \big\} = -\mathbf{E}\big\{ \frac{\partial^2\ l(\mathbf{X} | \theta)}{\partial \theta^2} \big\}$$

-----------------------------------------------------

## Fisher information and properties of MLE   

Fisher information leads to an important relationship    

- The MLE parameter estimate $\hat{\theta}$ is a Normally distributed random variable

- Arrises from the Taylor expansion of the maximum likelihood estimator   

$$
0 = \frac{\partial\ l(\hat{\theta})_\mathbf{X}}{\partial \theta} = 
\frac{\partial\ l(\theta)_\mathbf{X}}{\partial \theta} + 
\frac{\partial^2\ l(\hat{\theta})_\mathbf{X}}{\partial \theta^2} (\hat{\theta} - \theta)
$$
or     

$$
0 = l'(\theta)_\mathbf{X} + l''(\hat{\theta})_\mathbf{X}
$$

- $l'(\theta)_\mathbf{X} =$ first partial derivative  

- $l''(\hat{\theta})_\mathbf{X} =$ second partial derivative. 

-----------------------------------------------------

## Fisher information and properties of MLE   

Continuing with the simplified notation, and solving for $\hat{\theta}$;    

$$
\hat{\theta} = \theta + \frac{l'(\theta)_\mathbf{X}/n}{-l''(\hat{\theta})_\mathbf{X}/n}
$$

Fisher information relates to the score function as its variance   

$$
\frac{\partial\ l(\theta)}{ \partial \theta}\ \dot{\sim}\
\mathcal{N} \big(0, 1/ \mathcal{I}_\theta \big)
$$


For **large sample**, $n \rightarrow \infty$, take the expectation over $\mathbf{X}$. Assuming first and second derivatives exist and continuous, then by the Central Limit Theorem:    

$$\hat{\theta} \dot{\sim} \mathcal{N}\Big(\theta, \frac{1}{n\mathcal{I}(\theta)} \Big)$$

-----------------------------------------------------

## Fisher information and properties of MLE   

$$
\frac{\partial\ l(\theta)}{ \partial \theta}\ \dot{\sim}\
\mathcal{N} \big(0, 1/ \mathcal{I}_\theta \big)
$$

Relationship shows several important properties     

- The maximum likelihood estimate of model parameters, $\hat{\theta}$, is Normally distributed      
- The larger the Fisher information, the lower the variance of the parameter estimate    
  - Greater curvature of the log likelihood function gives more certain the parameter estimates       
  - The variance of the parameter estimate is inversely proportional to the number of samples, $n$.   

------------------------------

## Example of MLE for Normal distribution   

MLE for the Normal distribution   

- Find derivatives of the log-likelihood function with respect to the model parameters, $\mu$ and $\sigma2$    

$$
\begin{pmatrix}
\frac{\partial l}{\partial \mu} \\
\frac{\partial l}{\partial \sigma^2}
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{\sigma^2} \sum_j (x_j - \mu) \\
-\frac{n}{2 \sigma^2} + 
\frac{1}{2 \sigma^4} \sum_j (x_j - \mu)^2
\end{pmatrix}
=
\begin{pmatrix}
0 \\ 0
\end{pmatrix}
$$

Solving above for the estimate of the mean, $\bar{x}$  


$$
\sum_{j=1}n (x_j - \mu) = 0 
$$
$$
\bar{x} = \frac{1}{n} \sum_{j=1} x_j
$$
------------------------------

## Example of MLE for Normal distribution   

MLE for the Normal distribution   

$$
\begin{pmatrix}
\frac{\partial l}{\partial \mu} \\
\frac{\partial l}{\partial \sigma^2}
\end{pmatrix}
=
\begin{pmatrix}
\frac{1}{\sigma^2} \sum_j (x_j - \mu) \\
-\frac{n}{2 \sigma^2} + 
\frac{1}{2 \sigma^4} \sum_j (x_j - \mu)^2
\end{pmatrix}
=
\begin{pmatrix}
0 \\ 0
\end{pmatrix}
$$

Find the maximum likelihood estimate of $\sigma^2$    

$$
\frac{1}{s^2} \sum_{j=1} (x_j - \mu)^2 = n
$$
$$
s^2 = \frac{1}{n} \sum_{j=1}^n (x_j - \bar{y})^2 
$$


------------------------------

## Example of MLE for Normal distribution   



$$
\mathcal{J}(\vec{\theta}) = 
\begin{pmatrix}
\frac{\partial^2 l}{\partial \mu^2} & \frac{\partial^2 l}{\partial \mu\ \partial \sigma^2}\\
\frac{\partial^2 l}{\partial \mu\ \partial \sigma^2} & \frac{\partial^2 l}{\partial (\sigma^2)^2}
\end{pmatrix} 
= 
\begin{pmatrix}
-\frac{n}{\sigma^2} & -\frac{n}{\sigma^4}(\bar{x} - \mu) \\
-\frac{n}{\sigma^4}(\bar{x} - \mu) & -\frac{n}{2\sigma^4} + \frac{1}{\sigma^6} \sum_j (x_j - \mu)^2
\end{pmatrix}
= 
\begin{pmatrix}
-\frac{n}{\sigma^2} & 0 \\
0 & -\frac{n}{2\sigma^4} 
\end{pmatrix}
$$

The simplification results from the fact that $\bar{x} \rightarrow  \mathbf{E}(\mathbf{x_j})  = \mu$ and $s^2 \rightarrow \mathbf{E} \big\{ (x_j - \mu)^2 \big\} = \sigma^2$ in the limit of a large sample from the law of large numbers.   

There are some aspects of these relationships which make the MLE method attractive:     
- The curvature of the MLE for both parameters increases with the number of samples $n$. In other words, the peak of the log-likelihood function becomes better defined as $n$ increases.     
- The maximum likelihood estimates of the parameters, $\mu$ and $\sigma^2$ are independent. The off-diagonal terms are $0$.      






