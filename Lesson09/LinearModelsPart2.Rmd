---
title: "LinearModelsPart2"
author: "Steve Elston"
date: "03/06/2021"
output:
  slidy_presentation: default
  pdf_document: default
  beamer_presentation: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
use_python("/usr/bin/python3")
matplotlib <- import("matplotlib")
matplotlib$use("Agg", force = TRUE)
#knitr::knit_engines$set(python = reticulate::eng_python)
```

## Review    

Linear models are a flexible and widely used class of models   

- Fit model coefficients by **least squares** estimation   

- Can use many types of predictor variables   

- We prefer the simplest model that does a reasonable job   
   - The principle of **Ocam's razor**  

- Must consider the **bias-variance trade-off**  


## Review   

When evaluating any machine learning model consider **all evaluation methods available**    

- No one method best all of the time    
  - Homoskedastic Normally distributed residuals   
  - Reasonable values $R^2$, RMSE, etc
  - Are the model coefficients all significant? 

- **Different methods highlight different problems** with your model     

- Don't forget to check that the **model must make sense** for you application! 


## Review      

 Representation of machine learning models      
 
 - The key representation is the model matrix      
    - Column of 1s for intercept      
    - Columns of feature or predictor values   
 
 $$
A = 
\begin{bmatrix}
1, x_{1,1}, x_{1,2}, \ldots, x_{1,p}\\
1, x_{2,1}, x_{2,2}, \ldots, x_{2,p}\\
1, x_{3,1}, x_{3,2}, \ldots, x_{3,p}\\
\vdots,\ \vdots,\ \vdots,\ \vdots,\ \vdots\\
1, x_{n,1}, x_{n,2}, \ldots, x_{n,p}
\end{bmatrix}
$$
 
 - There are two standards for signatures of ML functions    
    - A model matrix $X$ (exogenous-features) and label array $Y$ (dependent-endogenous) - Scikit-learn and base Statsmodels   
    - A data frame with all features (predictors) and label (dependent) columns plus a model formula - Statsmodels formula and R   



## Review    

There are a number of assumptions in linear models that you overlook at your peril! 

- The feature or predictor variables should be **independent** of one another      
   - This is rarely true in practice   
   - **Multi-collinearity** between features makes the model **under-determined**   

- We assume that numeric features or predictors have **zero mean** and about the **same scale**      
   - We do not want to bias the estimation of regression coefficients with predictors that do not have a 0 mean   
   - We do not want to have predictors with a large numeric range dominate training   
   - Example: income is in the range of 10s or 100s of thousands and age is in the range of 10s, but apriori income is no more important than age as a predictor  
   
- Values of each predictor or feature should be iid      
   - If variance changes with sample, the optimal value of the coefficient could not be constant    
   - If there **serial correlation** in the predictor values, the iid assumption is violated - but can account for this such as in time series models   
   


## Review       

Scaling of features is required for many machine learning models      

- Several commonly used approaches      
   - **Z-score** scaling results in features with zero mean and unit variance    
   - Use Z-score scaling for features approximately normally distributed   
   - **Min-max** scaling transforms feature values to range 0-1   
   - Use min-max scaling for features with truncated range of values   

- Effect on model coefficients   
   - Scaling changes model coefficients by the scale factor applied   
   - Can re-scale (unscale) model coefficients before processing unknown cases    
   - Or use **same scaling** for unknown feature values   
   
- When coding categorical variables as binary dummy variables no need to scale - already in range [0-1]    


## Introduction  

- Working with categorical variables    
   - One-hot encoding  

- Over-fit models and regularization    
   - Bias variance trade-off
   - L2 regularization   
   - L1 regularization
   - Elastic Net  

- Models with nonlinear response   
   - The generalized linear model   
   - Link function transforms to linear model  



## Working with Categorical Variables 

Linear models, like nearly all machine learning models, use numeric features    

- How can categorical variables be used in linear models?    

- Need to transform categories to numeric variables with **one hot encoding**      
   - Each category becomes a binary **dummy variable**, encoded [0,1]  
   - Only one dummy variable has nonzero value - encodes the category   
   - n categories represented by n-1 dummy variables; all 0s encodes one level    
   
- Binary variables are an exception    
   - Repesent with single binay variable. [0,1] values


## Working with Categorical Variables  

Example: Consider a data set with categorical variables

```{python, echo=FALSE}
import pandas as pd
import numpy as np
import numpy.random as nr
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import scipy.stats as ss  
from math import atan

test_scores = pd.read_csv('../data/hsb2.csv', index_col=0)
test_scores.head(10)
```


## Working with Categorical Variables  

Example: We can encode a categorical variable with the Python patsy package to get the X (model) and Y(label) arrays:     

```{python}
from patsy import dmatrices
Y, X = dmatrices("socst ~ C(ses, levels=[1,2,3])", data=test_scores)
print(X[:5])
print(Y[:5])
```


## Working with Categorical Variables  

Example: A simple linear model with one categorical variable       

```{python}
import statsmodels.formula.api as smf 
linear_model = smf.ols("socst ~ C(ses)", data=test_scores).fit()
linear_model.summary()
```


## Working with Categorical Variables  

Wait! What happened to the coefficient for the first level of ses?  

- The intercept is the **mean response** of the first level     

- The other coefficients are **contrasts** with respect to mean of the first level.       
- Consider the following possible ways we can encode responses to a categorical variable - often called a **treatment**      
   - For $n$ treatments, there are mean responses $[ \mu_1, \mu_2, \ldots, \mu_n ]$  
   - The alternative encoding is a treatmentwith intercept, $I$, at $n - 1$ contrasts, $[ I, c_1, \ldots, c_{n-1}$    
   
- The means and contrasts are related:    
   
$$\begin{bmatrix}
I\\
c_2\\
\vdots\\
c_n
\end{bmatrix} = 
\begin{bmatrix}
\mu_1\\
\mu_2 - I\\
\vdots\\
\mu_n - I
\end{bmatrix}$$   

## Working with Categorical Variables    

In a linear model we can sometimes relate the coefficient values to an effect size    

- Start with $n$ treatments $[t_1, t_2, \ldots, t_n ]$ with effect sizes, $[e_1, e_2, \ldots, e_n ]$     

- With **no intercept term** the means represent the effect sizes:   

$$\begin{bmatrix}
e_1\\
e_2\\
\vdots\\
e_n
\end{bmatrix} = 
\begin{bmatrix}
\mu_1\\
\mu_2\\
\vdots\\
\mu_n
\end{bmatrix}$$  

- With intercept term compute effect sizes using contrasts:  

$$\begin{bmatrix}
e_1\\
e_2\\
\vdots\\
e_n
\end{bmatrix} = 
\begin{bmatrix}
I\\
I + c_1\\
\vdots\\
I + c_{n-1}
\end{bmatrix}$$   
 
 
## Working with Categorical Variables    

In a linear model we can sometimes relate the coefficient values to an effect size    

- Assumes the treatments are orthogonal    
   - In other words, applied on at a time    
   - e.g. a case can only be in one category     
   
- Assumes that the model coefficients are statistically independent    
   - Coefficients are dependent in overfit model    
   
- Often need to **adjust** for other effects    
   - Other treatments   
   - Levels of other categorical variables    
   - Use **partial slope** of continuous variables  
   
- In other words **apply** with care!    
   - Don't over-interpret your model   
   - Conditions in real world hard to verify, particularly for observational data   
 
 
## Dealing with Overfit Models

Example: We might suspect that the terms, ses and prog are significant in predicting social science test scores:  

```{python}
linear_model = smf.ols("socst ~ C(ses)*C(prog)", data=test_scores).fit()
linear_model.summary()
```


## Dealing with Overfit Models

Example: After 3 or 4 rounds of *guess and cut* feature punning, we arrive at a model will only siginficant coefficients:    

```{python}
linear_model = smf.ols("socst ~ - 1 + C(ses):C(prog)", data=test_scores).fit()
linear_model.summary()
```


## Dealing with Overfit Models   

Let's compare the results of the unpruned and pruned models     

- The metrics indicate the fit is exactly the same   
   - Why prefer the pruned model?    
   
- What are the consequence of the over-fit model?    
   - Several predictors (features) are included that are not needed     
   - Including non-significant predictors can only reduce generalization of the model    
   - Consider the effect of an unexpected value of a non-significant predictor on the response recalling the response is linear in the coefficients    
   
- But manually pruning a model with a great many features is a doomed task!   
   - What if we included all the interactions with type of school, race and sex?   
   - We now have up to 5th order interaction - 45 model coefficients with none significant!!    



## Dealing with Overfit Models   

**Idea!:** Try systematically pruning the model using some metric    

- But hard to find a good metric    

- Further, making multiple hypothesis tests is a fraught undertaking    
   - High probability of Type 1 or Type 2 error     
   - The multiple hypothesis problem    
   
- Leads to the the **step-wise regression algorithm**     
   - Forward step-wise regression adds most explanatory variable one at a time    
   - Backward step-wise regression removes lest explanatory variable one at a time    
   - Can go both directions - see the R documentation     


## Regularization - a Better Approach      

Regularization is a systematic approach to preventing over-fitting     

- To understand regularization need to understand the bias-variance trade-off     

- To better understand this trade-off decompose mean square error:    

$$\Delta y = E \big[ Y - \hat{f}(X) \big]$$
$y =$ the label vector     
$X =$ the feature matrix      
$\hat{f}(x) =$ the trained model      

- Expanding this relation gives us:    

$$\Delta y = \big( E[ \hat{f}(X)] - \hat{f}(X) \big)^2 + E \big[ ( \hat{f}(X) - E[ \hat{f}(X)])^2 \big] + \sigma^2\\
\Delta y = Bias^2 + Variance + Irreducible\ Error$$


## Regularization - a Better Approach

There is a trade-off between bias and variance      

- Need to find the trade-off point  

```{r BiasVariance, out.width = '50%', fig.cap='The trade-off between bias and variance', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/BiasVariance.png"))
```


## Regularization - a Better Approach

Decompose the $p$ x $p$ covariance matrix $cov(A) = \frac{1}{p}A^TA$ into **eigenvalues** and **eigenvectors**: 

$$cov(A) = Q \Lambda Q^{-1}$$

where,   

$Q$ is the $p$ x $p$ matrix of $p$ **orthonormal eigenvectors** of length $p$  

And, the $p$ eigenvales are represented as diagonal matrix: 

$$
\Lambda = 
\begin{bmatrix}
\lambda_1, 0, \ldots, 0\\
0, \lambda_2, \ldots \\
\vdots,\ \vdots,\ \vdots,\  \vdots\\
0, 0, \ldots, \lambda_p
\end{bmatrix}
$$

For real-valued $cov(A)$ the eigenvectors $Q$ are real valued so:   

$$Q^{-1} = Q^T$$


## Regularization - a Better Approach

The inverse of the covariance can be computed from its eigendecomposition   

$$cov(A)^{-1} = Q^T \Lambda^{-1} Q$$

Where $Q^{-1}$ is writen:       

$$
\Lambda^{-1} = 
\begin{bmatrix}
\frac{1}{\lambda_1}, 0, \ldots, 0\\
0, \frac{1}{\lambda_2}, \ldots \\
\vdots,\ \vdots,\ \vdots,\  \vdots\\
0, 0, \ldots, \frac{1}{\lambda_p}
\end{bmatrix}
$$

- But, if columns of $A$ are not linearly independent, the covariance matrix is **ill-posed** and the eigenvectors are under-determined       

- The eigenvalues are ordered, $\lambda_1 \ge \lambda_2 \ge \ldots \ge \lambda_p$

- For ill-posed covariance matrix    
   - The smallest $\lambda_i \rightarrow 0$   
   - So, $\frac{1}{\lambda_i} \rightarrow \infty$
   - The inverse covariance matrix does not exist!    


## L2 Regularization     

**Example:** compute the eigenvalues of a covariance matrix     

```{python}
test_scores['socst_zero_mean'] = test_scores['socst'] - np.mean(test_scores['socst'])
Y, X = dmatrices("socst_zero_mean ~ C(ses, levels=[1,2,3])*C(prog, levels=[1,2,3])", data=test_scores)
cov_X = np.matmul(np.transpose(X),X)
cov_X = np.divide(cov_X, float(cov_X.shape[0]))
np.real(np.linalg.eigvals(cov_X))
```

The condition number of the covariance is $\sim 100$

**Add regularization** with $\alpha=2.0$ and compute the eigenvalues

```{python}
alpha_sqr = 4.0 
alpha_sqr = np.diag([alpha_sqr] * cov_X.shape[0])
#alpha_sqr = np.diag([alpha] * cov_X.shape[0])
#cov_X = np.divide(np.matmul(np.transpose(X),X), float(cov_X.shape[0]))
cov_X = np.add(cov_X, alpha_sqr)
np.real(np.linalg.eigvals(cov_X))
```

The condition number of the covariance is $\sim 10$

## L2 Regularization    

L2 regularization constrains the Euclidean norm of the parameter vector, $\vec{b}$ 

- Recall:   

$$x = A b + \epsilon$$

- The **normal equations provide a solution**:

$$b = (A^TA)^{-1}A^Tx$$

- This solution requires finding the inverse of the covariance matrix, $(A^TA)^{-1}$       
   - But this inverse may be unstable   
   - In mathematical terminology we say the problem is **ill-posed**     
   
   
## L2 Regularization    

L2 regularization constrains the Euclidean norm of the parameter vector, $\vec{b}$ 

- We can add a **bias term** to the diagonal of the covariance matrix     

the **L2 or Euclidean norm** minimization problem):

$$min [\parallel A \cdot x - b \parallel +\ \alpha \parallel b\parallel]\\  or \\
b = (A^TA + \alpha^2 I)^{-1}A^Tx$$

Where the L2 norm of the coefficient vector is:
$$||b|| = \big( \beta_1^2 + \beta_2^2 + \ldots + \beta_m^2 \big)^{\frac{1}{2}} = \Big( \sum_{i=1}^m \beta_i^2 \Big)^{\frac{1}{2}}$$


## L2 Regularization    

How can we understand this relationship?     

$$b = (A^TA + \alpha^2I)^{-1}A^Tx$$

- Adds values along the diagonal of the covariance matrix     
   - This creates a so called ridge in the covariance, $cov_{regularized} = A^TA + \alpha^2$     
   - Leads to the term **ridge regression**   

- Constrain the L2 norm values of the model coefficients using the **penalty term** $\alpha$     
   - Larger $\alpha$ is more bias but lover variance   
   - Larger $\alpha$ makes the inverse of the covariance more stable   
   
- L2 regularization is a **soft constraint** on the model coefficients    
   - Even smallest coefficients are not driven to 0   
   - All coefficients can grow in value, but under the constraint   


## L2 Regularization   

Eigendecomposition of the regularized covariance matrix:    

$$A^TA + \alpha^2I = Q 
\begin{bmatrix}
\lambda_1 + \alpha^2, 0, \ldots, 0\\
0, \lambda_2 + \alpha^2, \ldots \\
\vdots,\ \vdots,\ \vdots,\  \vdots\\
0, 0, \ldots, \lambda_p + \alpha^2
\end{bmatrix}
Q^T$$

The inverse covariance matrix is then: 

$$A^TA + \alpha^2I = Q^T 
\begin{bmatrix}
\frac{1}{\lambda_1 + \alpha^2}, 0, \ldots, 0\\
0, \frac{1}{\lambda_2 + \alpha^2}, \ldots \\
\vdots,\ \vdots,\ \vdots,\  \vdots\\
0, 0, \ldots, \frac{1}{\lambda_p + \alpha^2}
\end{bmatrix}
Q$$

With any $\alpha > 0$, the inverse eigenvalues of the inverse covariance matrix are bounded     

Increasing $\alpha$ increases bias, but increases the stability of the inverse   

## L2 Regularization    

L2 regularization constrains the Euclidean norm of the parameter vector, $\vec{b}$    

- The norm of the coefficient vector, $\vec{b}$, is constrained


```{r L2, out.width = '60%', fig.cap='L2 norm constraint of model coefficients', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/L2.jpg"))
```   

## L2 Regularization   

Example: Increasing constraint on model coefficients with larger L2 regularization hyperparameter

```{python}
test_scores['socst_zero_mean'] = test_scores['socst'] - np.mean(test_scores['socst'])
def regularized_coefs(df, alphas, L1_wt=0.0, n_coefs=8,
                      formula = "socst_zero_mean ~ C(ses)*C(prog)",                                                     label='socst_zero_mean'):
    '''Function that computes a linear model for each value of the regualarization 
    parameter alpha and returns an array of the coefficient values. The L1_wt 
    determines the trade-off between L1 and L2 regualarization'''
    coefs = np.zeros((len(alphas),n_coefs + 1))
    MSE = []
    for i,alpha in enumerate(alphas):
        temp_mod = smf.ols(formula, data=df).fit_regularized(alpha=alpha,L1_wt=L1_wt)
        coefs[i,:] = temp_mod.params
        MSE.append(np.mean(np.square(df[label] - temp_mod.predict())))
    return coefs, MSE
alphas = np.arange(0.01, 10.0, step = 0.1)
Betas, MSE = regularized_coefs(test_scores, alphas)
Betas[:5]
```

## L2 Regularization   

Example: Increasing constraint on model coefficients with larger L2 regularization hyperparameter

```{python, echo=FALSE}
def plot_coefs(coefs, alphas, MSE, ylim=None):
    fig, ax = plt.subplots(1,2, figsize=(12, 5)) # define axis
    for i in range(coefs.shape[1]): # Iterate over coefficients
        ax[0].plot(alphas, coefs[:,i])
    ax[0].axhline(0.0, color='red', linestyle='--', linewidth=0.5)
    ax[0].set_ylabel('Partial slope values')
    ax[0].set_xlabel('alpha')
    ax[0].set_title('Parial slope values vs. regularization parameter number')
    if ylim is not None: ax[0].set_ylim(ylim)
    
    ax[1].plot(alphas, MSE)
    ax[1].set_ylabel('Mean squared error')
    ax[0].set_xlabel('alpha')
    ax[0].set_title('MSE vs. regularization parameter number')
    plt.show()
    
plot_coefs(Betas, alphas, MSE, ylim=(-5.0,5.0))
```

## L1 Regularization    

The L1 norm provides regularization with different properties     

- Constrains the model parameters using the L1 norm: 

$$min [\parallel A \cdot x - b \parallel +\ \alpha \parallel b\parallel^1]$$

- This form looks a lot like the L2 regularization formulation   

- $\parallel b\parallel^1$ is the L1 norm       

- Compute the l1 norm of the $m$ model parameters:

$$||b||^1 = \big( |b_1| + |b_2| + \ldots + |b_m| \big) = \Big( \sum_{i=1}^m |b_i| \Big)^1$$

$|b_i|$ is the absolute value of $b_i$. 


## L1 Regularization 

What are the properties of the L1 regularization   

- L1 norm is a **hard constraint**     

- L1 regularization drives coefficients to zero   

- The hard constraint property leads to the term **lasso regularization**    

- Lasso regression is a method of **feature selecttion**   


## L1 Regularization   

The Lasso regularization is a strong constraint on coefficient values   

- Some coefficients are forced to zero    

- The constraint curve is like a *lasso*


```{r L1, out.width = '60%', fig.cap='L1 norm constraint of model coefficients', fig.align='center', echo=FALSE}
knitr::include_graphics(rep("../images/L1.jpg"))
```   


## L1 Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python}
alphas = np.arange(0.01, 10.0, step = 0.1)
Betas, MSE = regularized_coefs(test_scores, alphas, L1_wt=1.0)
Betas[:5]
```


## L1 Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python, echo=FALSE}
plot_coefs(Betas, alphas, MSE, ylim=(-10.0,10.0))
```

## L1 Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python, echo=FALSE}
plot_coefs(Betas, alphas, MSE, ylim=(-1.0,3.0))
```



## Elastic Net Regularization    

Do we always have to choose between soft constraint of L2 and hard constraint of L1?   

- L2 regularization works well for **colinear features**    
   - Down-weights colinear features   
   - But soft constraint so poor model selection 

- L1 regularization provides **good model selection** by hard constraint    
   - But poor selection for colinear features     
   
- The **Elastic Net** weights L1 and L2 regularization    
   - Hyperparameter $\lambda$ weights L1 vs. L2 regularization   
   - Hyperparameter $\alpha$ sets strength of regularization  

$$min [\parallel A \cdot x - b \parallel +\ \lambda\ \alpha \parallel b\parallel^1]+\ (1- \lambda)\ \alpha \parallel b\parallel^2]$$

## Elastic Net Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python}
alphas = np.arange(0.01, 10.0, step = 0.1)
Betas, MSE = regularized_coefs(test_scores, alphas, L1_wt=0.5)
Betas[:5]
```


## Elastic Net Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python, echo=FALSE}
plot_coefs(Betas, alphas, MSE, ylim=(-10.0,8.0))
```

## Elastic Net Regularization   

Example: Increasing constraint on model coefficients with larger L1 regularization hyperparameter

```{python, echo=FALSE}
plot_coefs(Betas, alphas, MSE, ylim=(-1.0,3.0))
```

## Elastic Net Regularization 

Check the model summary for $\lambda=0.5$, $\alpha=1.5$

```{python}
lm_elastic = smf.ols("socst_zero_mean ~ C(ses)*C(prog)", data=test_scores).fit_regularized(alpha=1.5, L1_wt=0.5)
lm_elastic.params
```


```{python, echo=FALSE}
test_scores['predicted'] = lm_elastic.predict()
test_scores['resids'] = np.subtract(test_scores.predicted, test_scores.socst_zero_mean)

def residual_plot(df):
    plt.rc('font', size=12)
    fig, ax = plt.subplots(figsize=(8, 3), ) 
    RMSE = np.std(df.resids)
    sns.scatterplot(x='predicted', y='resids', data=df, ax=ax);
    plt.axhline(0.0, color='red', linewidth=1.0);
    plt.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0);
    plt.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0);
    plt.title('PLot of residuals vs. predicted');
    plt.xlabel('Predicted values');
    plt.ylabel('Residuals');
    plt.show()
    
residual_plot(test_scores)  
```



## Elastic Net Regularization 

Check the model summary for $\lambda=0.5$, $\alpha=3.0$

```{python}
lm_elastic = smf.ols("socst_zero_mean ~ C(ses)*C(prog)", data=test_scores).fit_regularized(alpha=3.0, L1_wt=0.5)
lm_elastic.params
```


```{python, echo=FALSE}
test_scores['predicted'] = lm_elastic.predict()
test_scores['resids'] = np.subtract(test_scores.predicted, test_scores.socst_zero_mean)

def residual_plot(df):
    plt.rc('font', size=12)
    fig, ax = plt.subplots(figsize=(8, 3), ) 
    RMSE = np.std(df.resids)
    sns.scatterplot(x='predicted', y='resids', data=df, ax=ax);
    plt.axhline(0.0, color='red', linewidth=1.0);
    plt.axhline(2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0);
    plt.axhline(-2.0*RMSE, color='red', linestyle='dashed', linewidth=1.0);
    plt.title('PLot of residuals vs. predicted');
    plt.xlabel('Predicted values');
    plt.ylabel('Residuals');
    plt.show()
    
residual_plot(test_scores)  
```


## Models with Nonlinear Response    

How do we deal with models that do not have nonlinear response variables?   

- Example: binary response variable, $[0,1]$ response    
   - A binary classifier      
   
- Intensity of an arrival process, $poisson(\lambda)$ response   

- The **generalized linear model (GLM)** is a framework for these models   
   
- For each distribution use a **link function** to transform to a linear model   
   - Find coefficients of transformed model with OLS!
   - Works for all exponential family response distributions     
   
   
## The Generalized Linear Model   

Start with a regression model for binary classification - **logistic regression**   

- Use Binomial distribution to model $y$ successes in $n$ trials    

$$y \sim Bi(n, \theta)$$

- But finding best fit value of parameter $\theta$ directly is a nonlinear problem!   
- Instead, define a **link function**, known as the **logit function** for parameter $\lambda$     

$$\lambda = log \Bigg\{ \frac{\theta}{1-\theta} \Bigg\}$$

- After a bit of algebra we find the exponential relationship with $y$

$$y = n\ log(1+ e^\lambda)$$


## The Generalized Linear Model  

Formulate a linear regression problem using the link function      

$$\lambda = log \Bigg\{ \frac{\theta}{1-\theta} \Bigg\} = X \vec{b}$$    
for model matrix $X$ and coefficient vector $\vec{b}$

- Given estimate of $\lambda$, compute $\theta$    

$$\theta = \frac{1}{1 + e^\lambda} = \frac{1}{1 + e^{X \vec{b}}}$$


## The Generalized Linear Model  

Consider the example for a single feature, $x$     

- The linear model has only slope and intercept, $[ \beta_0, \beta_1]$ 

$$\lambda_i = log \Bigg\{ \frac{\theta_i}{1-\theta_i} \Bigg\} = \beta_0 + \beta_1 x_i$$      

and    

$$\theta_i = \frac{1}{1 + e^{\beta_0 + \beta_1 x_i}}$$


## 

What does the transformation function look like?    

$$y = \frac{1}{1 + e^\lambda}$$

```{python, echo=FALSE}
# Plot the logistic transformation function (f(x) above)
x_seq = np.linspace(-7, 7, 100)

def log_fun(x, center=0, scale=1):
    e = np.exp(-scale*(x-center))
    log_out = 1./(1. + e)
    return log_out

log_fun_vectorized = np.vectorize(log_fun)

log_y = log_fun_vectorized(x_seq)

plt.plot(x_seq, log_y)
plt.title('Standard Logistic Function')
plt.xlabel('x')
plt.ylabel('y')
plt.grid()
plt.show()
```

- The response is bound in the range $[0,1]$

- We say the logistic transformation **squashes** the linear response $\lambda =X \vec{b}$ to binary, $[0,1]$   

- Can set a **decision threshold** for binary response    
   - Default $= 0.5$


## Evaluation of Classifiers   

How can we evaluate a classifier's accuracy? 

- Determine proportions of test cases which are classified as:     
   - True Positives (TP): Are positive and should be positive      
   - True Negatives (TN): Are negative and should be negative    
   - False Positives (FP): Classified as positive but are actually negative   
   - False Negatives (FN): Classified as negative but are actually positive   

- Organize these metrics into a **confusion matrix**      

|  | Classified Positive | Classified Negative |
| ---- | :---: | :---: |
|Positive | TP | FN |  
|Negative | FP | TN |



## Evaluation of Classifiers   


The other metrics are defined as follows:

- Accuracy = (TP + TN) / (TP + FP + TN + FN)      

- Selectivity or Precision = TP / (TP + FP)       
   - Precision is the fraction of the relevant class predictions are actually correct     
   
- Sensitivity or Recall = TP / (TP + FN)
   - Recall is the fraction of the relevant class were we able to predict    


- Is a trade-off between precision and recall     
   - Consider changing the decision threshold    
   - High threshold $\rightarrow$ higher recall, more false negatives   
   - Low threshold $\rightarrow$ higher precision, more false postives    

## Example of Logisitic Regression    

How well can we predict the type of school given the test scores?     

```{python}
## Prep the data
test_scores['schtyp'] = np.subtract(test_scores['schtyp'], 1)
for col in ['read', 'write', 'math', 'science', 'socst']:
    test_scores[col] = np.divide(np.subtract(test_scores[col], np.mean(test_scores[col])), np.std(test_scores[col]))

## Fit the model
formula = 'schtyp ~ math'
logistic_reg_model = smf.glm(formula=formula, data=test_scores, family=sm.families.Binomial()).fit()

## score the results 
threshold=0.18
test_scores['predicted'] = logistic_reg_model.predict()
test_scores['score'] = [1 if x>threshold else 0 for x in test_scores['predicted']]

print(logistic_reg_model.summary())
```


## Example of Logisitic Regression    

The data frame now looks like this with the predicted probability and the binary scores:

```{python}
test_scores.head(20)
```


## Example of Logisitic Regression    

Now, evaluate the model - the classifier is almost useless - **no Kagle awards!**:   

```{python}
import sklearn.metrics as sklm  
def print_metrics(labels, scores):
    metrics = sklm.precision_recall_fscore_support(labels, scores)
    conf = sklm.confusion_matrix(labels, scores)
    print('                 Confusion matrix')
    print('                 Score positive    Score negative')
    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])
    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])
    print('')
    print('Accuracy  %0.2f' % sklm.accuracy_score(labels, scores))
    print(' ')
    print('           Positive      Negative')
    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])
    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])
    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])
    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])
    
print_metrics(test_scores['schtyp'], test_scores['score'])    
```


## Summary  

- Working with categorical variables    
   - One-hot encoding creates binary numeric variables 
   - Only one case = 1, or hot, at a time  

- Over-fit models and regularization    
   - Bias variance trade-off between fit to training data and generalization 
   - L2 regularization is a soft constraint      
   - L1 regularization is a hard constraint  
   - Elastic Net trade-off between L1 and L2  

- Models with nonlinear response   
   - The generalized linear model   
   - Link function transforms to linear model  

